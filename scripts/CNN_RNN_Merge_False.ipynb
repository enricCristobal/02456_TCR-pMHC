{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a01d791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------- Import Libraries --------#\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import mlflow\n",
    "import gc\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3423c37a-4b59-4a1f-93f8-4ef70f490869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Modules from project--------#\n",
    "import encoding as enc\n",
    "from model import Net, Net_thesis, Net_project, Net_project_simple_CNN_RNN, Net_project_transformer_CNN_RNN\n",
    "import functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4211d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs available. Using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "#-------- Set Device --------#\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPUs available. Using CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5adc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Seeds --------#\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f33ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Directories --------#\n",
    "\n",
    "DATADIR = '/data/'\n",
    "TRAINDIR = '../data/train'\n",
    "VALIDATIONDIR = '../data/validation'\n",
    "MATRICES = '/data/Matrices'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f45575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train already unzipped.\n",
      "../data/validation already unzipped.\n",
      "Train directory:\n",
      "\n",
      " P2_labels.npz\n",
      "P3_input.npz\n",
      "P4_input.npz\n",
      "P2_input.npz\n",
      "__MACOSX\n",
      "P1_input.npz\n",
      "P3_labels.npz\n",
      "P4_labels.npz\n",
      "P1_labels.npz \n",
      "\n",
      "\n",
      "Validation directory:\n",
      "\n",
      " P5_input.npz\n",
      "P5_labels.npz\n",
      "__MACOSX\n"
     ]
    }
   ],
   "source": [
    "#-------- Unzip Train --------#\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(TRAINDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(TRAINDIR))\n",
    "except:\n",
    "    !unzip ../data/train.zip -d ../data/train\n",
    "\n",
    "    \n",
    "#-------- Unzip Validation --------#\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(VALIDATIONDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(VALIDATIONDIR))\n",
    "except:\n",
    "    !unzip ../data/validation.zip -d ../data/validation\n",
    "    \n",
    "print('Train directory:\\n\\n', '\\n'.join(str(p) for p in os.listdir(TRAINDIR)), '\\n\\n')\n",
    "print('Validation directory:\\n\\n','\\n'.join(str(p) for p in os.listdir(VALIDATIONDIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b37f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 5\n",
      "Size of file 0 1480\n",
      "Size of file 1 1532\n",
      "Size of file 2 1168\n",
      "Size of file 3 1526\n",
      "Size of file 4 1207\n"
     ]
    }
   ],
   "source": [
    "#-------- Import Dataset --------#\n",
    "\n",
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "import glob\n",
    "for fp in glob.glob(\"../data/train/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "for fp in glob.glob(\"../data/validation/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "data_partitions = len(data_list)\n",
    "\n",
    "print(\"Number of files:\", data_partitions)\n",
    "\n",
    "for i in range(data_partitions):\n",
    "    print(\"Size of file\", i, len(data_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c65246ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 0\n",
      "{1.0: 370, 0.0: 1110} \n",
      "\n",
      "File: 1\n",
      "{1.0: 383, 0.0: 1149} \n",
      "\n",
      "File: 2\n",
      "{1.0: 292, 0.0: 876} \n",
      "\n",
      "File: 3\n",
      "{1.0: 380, 0.0: 1146} \n",
      "\n",
      "File: 4\n",
      "{1.0: 301, 0.0: 906} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(target_list)):\n",
    "    print(\"File:\", i)\n",
    "    frequency = collections.Counter(target_list[i])\n",
    "    print(dict(frequency), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6876251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation = False\n",
    "merge = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9897fc5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#-------- Hyperparameters to fine tune -------#\n",
    "embedding = \"esm-1b\" #baseline\n",
    "numHN=64\n",
    "numFilter=100\n",
    "dropOutRate=0.1\n",
    "keep_energy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be048917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \n",
      "\n",
      "File 0\n",
      "MHC: ['GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETL']\n",
      "MHC length range: [179]\n",
      "PEP length range: [9]\n",
      "TCR length range: [200, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 228]\n",
      "\n",
      "\n",
      "File 1\n",
      "MHC: ['GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETL']\n",
      "MHC length range: [179]\n",
      "PEP length range: [9]\n",
      "TCR length range: [201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]\n",
      "\n",
      "\n",
      "File 2\n",
      "MHC: ['GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETL']\n",
      "MHC length range: [179]\n",
      "PEP length range: [9]\n",
      "TCR length range: [202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]\n",
      "\n",
      "\n",
      "File 3\n",
      "MHC: ['GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETL']\n",
      "MHC length range: [179]\n",
      "PEP length range: [9]\n",
      "TCR length range: [200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]\n",
      "\n",
      "\n",
      "File 4\n",
      "MHC: ['GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETL']\n",
      "MHC length range: [179]\n",
      "PEP length range: [9]\n",
      "TCR length range: [202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(data_list), \"\\n\")\n",
    "\n",
    "for i in range(len(data_list)):\n",
    "    print(\"File\", i)\n",
    "    seq = func.extract_sequences(data_list[i])\n",
    "\n",
    "    seq['PEP_len'] = seq['peptide'].str.len()\n",
    "    seq['TCR_len'] = seq['TCR'].str.len()\n",
    "    seq['MHC_len'] = seq['MHC'].str.len()\n",
    "\n",
    "    print(\"MHC:\", list(set(seq['MHC'].tolist())))\n",
    "    leng_TCR = list()\n",
    "    leng_MHC = list()\n",
    "    leng_PEP = list()\n",
    "\n",
    "    for i in range(len(seq)):\n",
    "\n",
    "        if seq['PEP_len'][i] not in leng_PEP:\n",
    "            leng_PEP.append(seq['PEP_len'][i])\n",
    "\n",
    "        if seq['MHC_len'][i] not in leng_MHC:\n",
    "            leng_MHC.append(seq['MHC_len'][i]) \n",
    "\n",
    "        if seq['TCR_len'][i] not in leng_TCR:\n",
    "            leng_TCR.append(seq['TCR_len'][i])\n",
    "\n",
    "    print(\"MHC length range:\", sorted(leng_MHC))\n",
    "    print(\"PEP length range:\",sorted(leng_PEP))\n",
    "    print(\"TCR length range:\",sorted(leng_TCR))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2506f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_list_aa = list()\n",
    "\n",
    "#for i in range(5):\n",
    "#    data_list_aa.append(func.extract_aa_and_energy_terms(data_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "852a50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of 1 complex:\n",
    "#pd.DataFrame(data_list_aa[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d8046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm-1b\n",
      "skipped\n",
      "\n",
      "\n",
      "skipped\n",
      "\n",
      "\n",
      "skipped\n",
      "\n",
      "\n",
      "\n",
      "Working on file 4\n",
      "Sequences are extracted\n",
      "Merge false\n",
      "1526 1526 1526\n",
      "\n",
      "Flag 0  - Time: 23.79\n"
     ]
    }
   ],
   "source": [
    "#embedding of data\n",
    "data_list_enc_mhc = list()\n",
    "data_list_enc_pep = list() \n",
    "data_list_enc_tcr = list()\n",
    "start = time.time()\n",
    "mhc_bool = False\n",
    "\n",
    "#create directory to fetch/store embedded\n",
    "embedding_dir= '../data/embeddedFiles/'\n",
    "\n",
    "\n",
    "data_list_enc = []\n",
    "if embedding == \"baseline\":\n",
    "    print('baseline')\n",
    "    data_list_enc = data_list\n",
    "\n",
    "elif embedding == \"esm-1b\":\n",
    "    print(\"esm-1b\")\n",
    "    count = 0\n",
    "\n",
    "    for dataset in data_list:\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if count < 4:\n",
    "            print(\"skipped\\n\\n\")\n",
    "            continue\n",
    "\n",
    "        print(\"\\nWorking on file\", count)\n",
    "        mhc_enc = list()\n",
    "        pep_enc = list()\n",
    "        tcr_enc = list()\n",
    "\n",
    "        x_enc = func.extract_sequences(dataset, merge=merge)\n",
    "        print(\"Sequences are extracted\")\n",
    "\n",
    "        if merge:\n",
    "            encoded = list()\n",
    "            print(\"Merge true\")\n",
    "            encoded = enc.esm_1b(x_enc['all'].tolist(), pooling=False)\n",
    "            for i in range(len(x_enc['all'].tolist())):\n",
    "                if i % 100 == 0:\n",
    "                    print(\"\\nFlag\", i, \" - Time:\", round(time.time()-start,2))\n",
    "                    all_i = enc.esm_1b_peptide(x_enc['all'].tolist()[i], pooling=False)\n",
    "                    encoded.append(all_i)\n",
    "                    \n",
    "            x_enc = np.array(x_enc[0])\n",
    "            print(x_enc.shape)\n",
    "            data_list_enc.append(x_enc)\n",
    "            #save\n",
    "            outfile = open(embedding_dir + 'dataset-all-{}-file{}-500'.format(embedding, count),'wb')\n",
    "            pickle.dump(data_list_enc, outfile)\n",
    "            outfile.close()\n",
    "\n",
    "        else:\n",
    "            print(\"Merge false\")\n",
    "            print(len(x_enc['MHC'].tolist()), len(x_enc['peptide'].tolist()), len(x_enc['TCR'].tolist()))\n",
    "\n",
    "            if mhc_bool == False:\n",
    "                mhc_enc_1 = enc.esm_1b_peptide(x_enc['MHC'].tolist()[0], pooling=False)\n",
    "                mhc_bool == True\n",
    "\n",
    "            for i in range(len(x_enc['MHC'].tolist())):\n",
    "                if i % 100 == 0:\n",
    "                    print(\"\\nFlag\", i, \" - Time:\", round(time.time()-start,2))\n",
    "                    \n",
    "                if i % 800 == 0 and i != 0:\n",
    "                    outfilemhc = open(embedding_dir + 'dataset-{}-file{}-mhc-{}.pkl'.format(embedding, count, i),'wb')\n",
    "                    outfilepep = open(embedding_dir + 'dataset-{}-file{}-pep-{}.pkl'.format(embedding, count, i),'wb')\n",
    "                    outfiletcr = open(embedding_dir + 'dataset-{}-file{}-tcr-{}.pkl'.format(embedding, count, i),'wb')\n",
    "                    pickle.dump(mhc_enc, outfilemhc)\n",
    "                    pickle.dump(pep_enc, outfilepep)\n",
    "                    pickle.dump(tcr_enc, outfiletcr)\n",
    "                    outfilemhc.close()\n",
    "                    outfilepep.close()\n",
    "                    outfiletcr.close()\n",
    "\n",
    "                mhc_enc.append(mhc_enc_1)\n",
    "                pep_enc.append(enc.esm_1b_peptide(x_enc['peptide'].tolist()[i], pooling=False))\n",
    "                tcr_enc.append(enc.esm_1b_peptide(x_enc['TCR'].tolist()[i], pooling=False))\n",
    "\n",
    "            mhc_enc = [x.tolist() for x in mhc_enc]\n",
    "            pep_enc = [x.tolist() for x in pep_enc]\n",
    "            tcr_enc = [x.tolist() for x in tcr_enc]\n",
    "\n",
    "            #print(\"results are stacking\")\n",
    "            #x_enc = np.vstack((mhc_enc[0],pep_enc[0],tcr_enc[0]))\n",
    "            #print(\"before extending:\", x_enc.shape)\n",
    "\n",
    "            # x_enc should be enc + energy terms\n",
    "            data_list_enc_mhc.append(mhc_enc) \n",
    "            data_list_enc_pep.append(pep_enc) \n",
    "            data_list_enc_tcr.append(tcr_enc) \n",
    "            print(\"ESM_1B is done\\n\")\n",
    "\n",
    "            #save\n",
    "            outfilemhc = open(embedding_dir + 'dataset-{}-file{}-mhc.pkl'.format(embedding, count),'wb')\n",
    "            outfilepep = open(embedding_dir + 'dataset-{}-file{}-pep.pkl'.format(embedding, count),'wb')\n",
    "            outfiletcr = open(embedding_dir + 'dataset-{}-file{}-tcr.pkl'.format(embedding, count),'wb')\n",
    "            pickle.dump(mhc_enc, outfilemhc)\n",
    "            pickle.dump(pep_enc, outfilepep)\n",
    "            pickle.dump(tcr_enc, outfiletcr)\n",
    "            outfilemhc.close()\n",
    "            outfilepep.close()\n",
    "            outfiletcr.close()\n",
    "\n",
    "elif embedding == \"esm_ASM\":\n",
    "    for dataset in data_list:\n",
    "        x_enc = func.extract_sequences(dataset, merge=merge).values.tolist()\n",
    "        print(data_list_enc)\n",
    "        x_enc = [enc.esm_ASM(seq, pooling=False) for seq in x_enc]\n",
    "        data_list_enc.append(x_enc)\n",
    "\n",
    "    #save\n",
    "    outfile = open(embedding_dir + 'dataset-{}-file{}'.format(embedding, count),'wb')\n",
    "    pickle.dump(data_list_enc, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "else:         \n",
    "    for dataset in data_list:\n",
    "        x_enc = func.extract_sequences(dataset, merge=merge).values.tolist()\n",
    "        print(data_list_enc)\n",
    "        x_enc = [enc.encodePeptidesCNN(seq, scheme=embedding) for seq in x_enc]\n",
    "        data_list_enc.append(x_enc)\n",
    "    print(data_list_enc)\n",
    "\n",
    "    #save\n",
    "    outfile = open(embedding_dir + 'dataset-{}-file{}'.format(embedding, count),'wb')\n",
    "    pickle.dump(data_list_enc, outfile)\n",
    "    outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f975587",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"esm-1b\":\n",
    "    pd.DataFrame(data_list_enc_tcr[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e438631",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"baseline\":\n",
    "    print(\"baseline\")\n",
    "    print(len(data_list_enc), \"\\n\")\n",
    "\n",
    "    for i in range(len(data_list)):\n",
    "        print(\"number of complexes:\", len(data_list[i]))\n",
    "        print(\"number of rows:\", len(data_list[i][0]))\n",
    "        print(\"number of columns:\", len(data_list[i][0][0]))\n",
    "        print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"ESM 1B:\")\n",
    "    final_cmplx\n",
    "\n",
    "    print(len(data_list_enc), \"\\n\")\n",
    "\n",
    "    for i in range(len(data_list_enc)):\n",
    "        print(\"number of complexes:\", len(data_list_enc[i]))\n",
    "        print(\"number of rows:\", len(data_list_enc[i][0]))\n",
    "        print(\"number of columns:\", len(data_list_enc[i][0][0]))\n",
    "        print(\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed hyperparameters\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "bat_size = 128\n",
    "epochs = 100\n",
    "riterion = nn.BCEWithLogitsLoss()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate(data_list_enc[0:3])\n",
    "y_train = np.concatenate(target_list[0:3])\n",
    "nsamples, nx, ny = X_train.shape\n",
    "print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "X_valid = np.concatenate(data_list_enc[3:4])\n",
    "y_valid = np.concatenate(target_list[3:4])\n",
    "nsamples, nx, ny = X_valid.shape\n",
    "print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "\n",
    "X_test = np.concatenate(data_list_enc[3:4])\n",
    "y_test = np.concatenate(target_list[3:4])\n",
    "nsamples, nx, ny = X_test.shape\n",
    "print(\"Test set shape:\", nsamples,nx,ny)\n",
    "\n",
    "# features and residues\n",
    "features = list(range(ny))\n",
    "residues = list(range(nx)) \n",
    "n_features = len(features)\n",
    "input_size = len(residues)\n",
    "\n",
    "# Dataloader\n",
    "train_ds = []\n",
    "for i in range(len(X_train)):\n",
    "    train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "val_ds = []\n",
    "for i in range(len(X_valid)):\n",
    "    val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "test_ds = []\n",
    "for i in range(len(X_valid)):\n",
    "    test_ds.append([np.transpose(X_test[i][:,features]), y_test[i]])\n",
    "    \n",
    "    \n",
    "train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)\n",
    "test_ldr = torch.utils.data.DataLoader(test_ds,batch_size=bat_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "###    CNN+RNN (thesis)     ###\n",
    "###############################\n",
    "\n",
    "if cross_validation == False:\n",
    "\n",
    "    #-------- Train --------#\n",
    "    nsamples, nx, ny = X_train.shape\n",
    "    print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "    nsamples, nx, ny = X_valid.shape\n",
    "    print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "    # Dataloader\n",
    "    train_ds = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "    val_ds = []\n",
    "    for i in range(len(X_valid)):\n",
    "        val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "    train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=len(train_ds), shuffle=True)\n",
    "    val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)\n",
    "\n",
    "    # Initialize network\n",
    "    net = Net_project_simple_CNN_RNN(num_classes=num_classes, \n",
    "             n_features=n_features, \n",
    "             numHN=numHN, \n",
    "             numFilter=numFilter,\n",
    "             dropOutRate=dropOutRate).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate,\n",
    "                           weight_decay=0.0005,\n",
    "                           amsgrad=True,)\n",
    "    \n",
    "    train_acc, train_losses, train_auc, valid_acc, valid_losses, valid_auc, val_preds, val_targs = func.train_project(net, optimizer, train_ldr, val_ldr, test_ldr, X_valid, epochs, criterion, early_stop)\n",
    "\n",
    "\n",
    "    #-------- Performance --------#\n",
    "    epoch = np.arange(1,len(train_losses)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_losses, 'r', epoch, valid_losses, 'b')\n",
    "    plt.legend(['Train Loss','Validation Loss'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "\n",
    "    epoch = np.arange(1,len(train_auc)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_auc, 'r', epoch, valid_auc, 'b')\n",
    "    plt.legend(['Train AUC','Validation AUC'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('AUC')\n",
    "\n",
    "    epoch = np.arange(1,len(train_acc)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "    plt.legend(['Train Accuracy','Validation Accuracy'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('Acc')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    #-------- Save results --------#\n",
    "\n",
    "    results = pd.DataFrame(list(zip( (int(x) for x in val_targs), (int(x) for x in val_preds))),columns =['target', 'pred'])\n",
    "    print(results)\n",
    "\n",
    "    #results.to_csv('results/df_targets_preds_th.csv'.format(str(date.today())), index=False)\n",
    "    \n",
    "    \n",
    "    #-------- Performance Evaluation --------#\n",
    "    # The results change every time we train, we should check why (maybe we missed something or did wrong with the seeds?)\n",
    "\n",
    "    print(\"AUC: \", roc_auc_score(results['target'], results['pred']))\n",
    "    print(\"MCC: \", matthews_corrcoef(results['target'], results['pred']))\n",
    "\n",
    "    confusion_matrix = pd.crosstab(results['target'], results['pred'], rownames=['Actual'], colnames=['Predicted'])\n",
    "    sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot roc curve\n",
    "\n",
    "    fpr, tpr, thres = roc_curve(results['target'], results['pred'])\n",
    "    print('AUC: {:.3f}'.format(roc_auc_score(results['target'], results['pred'])))\n",
    "\n",
    "    print( len([i for i, (a, b) in enumerate(zip(results['pred'], results['target'])) if a != b]))\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    # roc curve\n",
    "    plt.plot(fpr, tpr, \"b\", label='ROC Curve')\n",
    "    plt.plot([0,1],[0,1], \"k--\", label='Random Guess')\n",
    "    plt.xlabel(\"false positive rate\")\n",
    "    plt.ylabel(\"true positive rate\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"ROC curve\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    AUC = roc_auc_score(results['target'], results['pred'])\n",
    "    MCC = matthews_corrcoef(results['target'], results['pred'])\n",
    "    print(\"AUC: \", AUC)\n",
    "    print(\"MCC: \", MCC)\n",
    "    print(\"early stop:\", early_stop)\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing values\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('embedding', embedding) \n",
    "    mlflow.log_param('Hidden Neurons', numHN)\n",
    "    mlflow.log_param('filters CNN', numFilter)\n",
    "    mlflow.log_param('Dropout rate', dropOutRate)\n",
    "    mlflow.log_metric('AUC', AUC)\n",
    "    mlflow.log_metric('MCC', MCC)\n",
    "    #ADD ARTIFACTS (PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebcc79-4feb-4f4f-bf49-0271c1dcacac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e764bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2802b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
