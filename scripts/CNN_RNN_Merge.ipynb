{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a01d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Libraries --------#\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import mlflow\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3423c37a-4b59-4a1f-93f8-4ef70f490869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Modules from project--------#\n",
    "#import encoding as enc\n",
    "from model import Net, Net_thesis, Net_project, Net_project_simple_CNN_RNN, Net_project_transformer_CNN_RNN\n",
    "#import functions as func\n",
    "\n",
    "### !!!!!!!!  I need to send new functions and encodings files to you, but for now:\n",
    "def esm_1b_peptide(peptide, pooling=True):\n",
    "    peptides = [peptide]\n",
    "    embeddings = list()\n",
    "    # Load pre-trained ESM-1b model\n",
    "\n",
    "    model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    data = []\n",
    "    count = 0\n",
    "    for peptide in peptides:\n",
    "        data.append((\"\", peptide))\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[12], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][12]\n",
    "    sequence_representations = []\n",
    "    del results, batch_labels, batch_strs, batch_tokens, model, alphabet, batch_converter\n",
    "    gc.collect()\n",
    "    for i, (_, seq) in enumerate(data):\n",
    "        count += 1\n",
    "        if count % 100 == 0:\n",
    "            print(\"\\t\\tFlag\", count)\n",
    "        if pooling:\n",
    "            return token_representations[i, 1: len(seq) + 1].mean(0)\n",
    "        else:\n",
    "            return token_representations[i, 1: len(seq) + 1]\n",
    "\n",
    "        \n",
    "def extract_sequences(dataset_X, merge=False):\n",
    "    \"\"\"\n",
    "    Return DataFrame with MHC, peptide and TCR a/b sequences from\n",
    "    one-hot encoded complex sequences in dataset X\n",
    "    \"\"\"\n",
    "    mhc_sequences = [reverseOneHot(arr[0:179,0:20]) for arr in dataset_X]\n",
    "    pep_sequences = [reverseOneHot(arr[179:192,0:20]) for arr in dataset_X] ## 190 or 192 ????\n",
    "    tcr_sequences = [reverseOneHot(arr[192:,0:20]) for arr in dataset_X]\n",
    "    all_sequences = [reverseOneHot(arr[:,0:20]) for arr in dataset_X]\n",
    "\n",
    "    if merge:\n",
    "        df_sequences = pd.DataFrame({\"all\": all_sequences})\n",
    "\n",
    "    else:\n",
    "        df_sequences = pd.DataFrame({\"MHC\":mhc_sequences,\n",
    "                                 \"peptide\":pep_sequences,\n",
    "                                 \"TCR\":tcr_sequences})\n",
    "        \n",
    "    return df_sequences\n",
    "    \n",
    "\n",
    "def extract_aa_and_energy_terms(dataset_X):\n",
    "    new_dataset_X = list()\n",
    "    for cmplx in range(len(dataset_X)):\n",
    "        df = pd.DataFrame(dataset_X[cmplx])\n",
    "        df['aa'] = 'A'\n",
    "        for i in range(len(df)):\n",
    "            df['aa'][i] = return_aa(list(df.iloc[i,0:20]))\n",
    "        df = df.iloc[:,20:]\n",
    "        new_dataset_X.append(np.array(df))\n",
    "    return np.array(new_dataset_X)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4211d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs available. Using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "#-------- Set Device --------#\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPUs available. Using CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5adc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Seeds --------#\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f33ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Directories --------#\n",
    "\n",
    "DATADIR = '/data/'\n",
    "TRAINDIR = '../data/train'\n",
    "VALIDATIONDIR = '../data/validation'\n",
    "MATRICES = '/data/Matrices'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f45575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train already unzipped.\n",
      "../data/validation already unzipped.\n",
      "Train directory:\n",
      "\n",
      " P2_labels.npz\n",
      "P3_input.npz\n",
      "P4_input.npz\n",
      "P2_input.npz\n",
      "__MACOSX\n",
      "P1_input.npz\n",
      "P3_labels.npz\n",
      "P4_labels.npz\n",
      "P1_labels.npz \n",
      "\n",
      "\n",
      "Validation directory:\n",
      "\n",
      " P5_input.npz\n",
      "P5_labels.npz\n",
      "__MACOSX\n"
     ]
    }
   ],
   "source": [
    "#-------- Unzip Train --------#\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(TRAINDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(TRAINDIR))\n",
    "except:\n",
    "    !unzip ../data/train.zip -d ../data/train\n",
    "\n",
    "    \n",
    "#-------- Unzip Validation --------#\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(VALIDATIONDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(VALIDATIONDIR))\n",
    "except:\n",
    "    !unzip ../data/validation.zip -d ../data/validation\n",
    "    \n",
    "print('Train directory:\\n\\n', '\\n'.join(str(p) for p in os.listdir(TRAINDIR)), '\\n\\n')\n",
    "print('Validation directory:\\n\\n','\\n'.join(str(p) for p in os.listdir(VALIDATIONDIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b37f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 5\n",
      "Size of file 0 1480\n",
      "Size of file 1 1532\n",
      "Size of file 2 1168\n",
      "Size of file 3 1526\n",
      "Size of file 4 1207\n"
     ]
    }
   ],
   "source": [
    "#-------- Import Dataset --------#\n",
    "\n",
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "import glob\n",
    "for fp in glob.glob(\"../data/train/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "for fp in glob.glob(\"../data/validation/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "data_partitions = len(data_list)\n",
    "\n",
    "print(\"Number of files:\", data_partitions)\n",
    "\n",
    "for i in range(data_partitions):\n",
    "    print(\"Size of file\", i, len(data_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c65246ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 0\n",
      "{1.0: 370, 0.0: 1110} \n",
      "\n",
      "File: 1\n",
      "{1.0: 383, 0.0: 1149} \n",
      "\n",
      "File: 2\n",
      "{1.0: 292, 0.0: 876} \n",
      "\n",
      "File: 3\n",
      "{1.0: 380, 0.0: 1146} \n",
      "\n",
      "File: 4\n",
      "{1.0: 301, 0.0: 906} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(target_list)):\n",
    "    print(\"File:\", i)\n",
    "    frequency = collections.Counter(target_list[i])\n",
    "    print(dict(frequency), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6876251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation = False\n",
    "merge = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9897fc5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#-------- Hyperparameters to fine tune -------#\n",
    "embedding = \"esm-1b\" #baseline\n",
    "numHN=64\n",
    "numFilter=100\n",
    "dropOutRate=0.1\n",
    "keep_energy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be048917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Working on file 1 - size: 2\n",
      "Merge true\n",
      "Sequences are extracted\n",
      "\n",
      "Flag 0  - Time: 0.02\n",
      "Encoding is done - encoded size: 2\n",
      "Save in file\n",
      "Saving is done.\n",
      "\n",
      "\n",
      "\n",
      "Working on file 2 - size: 2\n",
      "Merge true\n",
      "Sequences are extracted\n",
      "\n",
      "Flag 0  - Time: 53.8\n",
      "Encoding is done - encoded size: 2\n",
      "Save in file\n",
      "Saving is done.\n",
      "\n",
      "\n",
      "\n",
      "Working on file 3 - size: 2\n",
      "Merge true\n",
      "Sequences are extracted\n",
      "\n",
      "Flag 0  - Time: 102.14\n",
      "Encoding is done - encoded size: 2\n",
      "Save in file\n",
      "Saving is done.\n",
      "\n",
      "\n",
      "\n",
      "Working on file 4 - size: 2\n",
      "Merge true\n",
      "Sequences are extracted\n",
      "\n",
      "Flag 0  - Time: 151.81\n",
      "Encoding is done - encoded size: 2\n",
      "Save in file\n",
      "Saving is done.\n",
      "\n",
      "\n",
      "\n",
      "Working on file 5 - size: 2\n",
      "Merge true\n",
      "Sequences are extracted\n",
      "\n",
      "Flag 0  - Time: 198.3\n",
      "Encoding is done - encoded size: 2\n",
      "Save in file\n",
      "Saving is done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-------- Encode ALL --------#\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "count=0\n",
    "\n",
    "embedding_dir= '../data/embeddedFiles/'\n",
    "\n",
    "data_list_new = list()\n",
    "\n",
    "for dataset in data_list:\n",
    "\n",
    "    dataset = dataset[0:2] ######### !!!!!!!!!!  delete this -> [0:2]\n",
    "    count += 1\n",
    "\n",
    "    print(\"\\nWorking on file\", count, \"- size:\", len(dataset))\n",
    "    encoded = list()\n",
    "\n",
    "    print(\"Merge true\")\n",
    "    x_enc = extract_sequences(dataset, merge=True)\n",
    "    print(\"Sequences are extracted\")\n",
    "\n",
    "    for i in range(len(x_enc['all'].tolist())):\n",
    "        if i % 2 == 0:\n",
    "            print(\"Flag\", i, \" - Time:\", round(time.time()-start,2))\n",
    "        all_i = esm_1b_peptide(x_enc['all'].tolist()[i], pooling=False)\n",
    "        encoded.append(all_i)\n",
    "        \n",
    "    data_list_new.append(encoded)\n",
    "    print(\"Encoding is done - encoded size:\", len(encoded))\n",
    "    print(\"Save in file\")\n",
    "    outfile = open(embedding_dir + 'datasetall-{}-file{}'.format(\"esm1b\", count), 'wb')\n",
    "    pickle.dump(encoded, outfile)\n",
    "    outfile.close()\n",
    "    print(\"Saving is done.\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4642913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Paste Energy Terms Next to it --------#  ########### !!! May require debugging\n",
    "\n",
    "if embedding == \"esm-1b\":\n",
    "    data_list_enc = list()\n",
    "    \n",
    "    for t in range(len(data_list)):\n",
    "        print(t)\n",
    "        data_list_t = list()\n",
    "        for cmplx in range(len(data_list[t])):\n",
    " \n",
    "            df = pd.DataFrame( data_list[t][cmplx] ) #### baseline data to extract enery \n",
    "            x_df = pd.DataFrame( extract_aa_and_energy_terms(data_list[t])[cmplx] ) #### baseline data to find all 0s (paddings) \n",
    "            embedding_complex = pd.DataFrame(data_list_new[t][cmplx]) ##### new embeded data\n",
    "            \n",
    "            pad_index_list = sorted(x_df[x_df.iloc[:,34]=='X'].index.tolist())\n",
    "            \n",
    "            padding1_len = 0\n",
    "            for pad in range(len(pad_index_list)):\n",
    "                padding1_len += 1\n",
    "                if pad_index_list[pad+1]-pad_index_list[pad] > 100:\n",
    "                    tcr_start = pad_index_list[pad] + 1\n",
    "                    tcr_end = pad_index_list[pad+1]\n",
    "                    break\n",
    "            padding2_len = len(pad_index_list) - padding1_len\n",
    "\n",
    "            mhc = pd.concat([df.iloc[:179,20:].reset_index(drop=True), ///embedding_complex[??] here put mhc part of your embedding -> 0:179/// ], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "            pep = pd.concat([df.iloc[179:188,20:].reset_index(drop=True), ///embedding_complex[??] peptide part of your embedding -> 179:179+9/// ], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "            tcr = pd.concat([(df.iloc[tcr_start:tcr_end,20:]).reset_index(drop=True), ///embedding_complex[??] tcr partofyourembedding -> tcrstart: tcrend/// ], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "            padding = pd.DataFrame(0, index=np.arange(padding1_len + padding2_len), columns=pep.columns)\n",
    "\n",
    "            final_cmplx = pd.concat([mhc,pep,tcr,padding]).reset_index(drop=True)\n",
    "            final_cmplx.columns = np.arange(len(final_cmplx.columns))\n",
    "            data_list_t.append(np.array(final_cmplx))\n",
    "\n",
    "        data_list_enc.append(np.array(data_list_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53292ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"esm-1b\":\n",
    "    print(final_cmplx.to_markdown()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e438631",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"baseline\":\n",
    "    print(\"baseline\")\n",
    "    print(len(data_list_enc), \"\\n\")\n",
    "\n",
    "    for i in range(len(data_list)):\n",
    "        print(\"number of complexes:\", len(data_list[i]))\n",
    "        print(\"number of rows:\", len(data_list[i][0]))\n",
    "        print(\"number of columns:\", len(data_list[i][0][0]))\n",
    "        print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"ESM 1B:\")\n",
    "    final_cmplx\n",
    "\n",
    "    print(len(data_list_enc), \"\\n\")\n",
    "\n",
    "    for i in range(len(data_list_enc)):\n",
    "        print(\"number of complexes:\", len(data_list_enc[i]))\n",
    "        print(\"number of rows:\", len(data_list_enc[i][0]))\n",
    "        print(\"number of columns:\", len(data_list_enc[i][0][0]))\n",
    "        print(\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed hyperparameters\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "bat_size = 128\n",
    "epochs = 100\n",
    "riterion = nn.BCEWithLogitsLoss()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate(data_list_enc[0:3])\n",
    "y_train = np.concatenate(target_list[0:3])\n",
    "nsamples, nx, ny = X_train.shape\n",
    "print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "X_valid = np.concatenate(data_list_enc[3:4])\n",
    "y_valid = np.concatenate(target_list[3:4])\n",
    "nsamples, nx, ny = X_valid.shape\n",
    "print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "\n",
    "X_test = np.concatenate(data_list_enc[3:4])\n",
    "y_test = np.concatenate(target_list[3:4])\n",
    "nsamples, nx, ny = X_test.shape\n",
    "print(\"Test set shape:\", nsamples,nx,ny)\n",
    "\n",
    "# features and residues\n",
    "features = list(range(ny))\n",
    "residues = list(range(nx)) \n",
    "n_features = len(features)\n",
    "input_size = len(residues)\n",
    "\n",
    "# Dataloader\n",
    "train_ds = []\n",
    "for i in range(len(X_train)):\n",
    "    train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "val_ds = []\n",
    "for i in range(len(X_valid)):\n",
    "    val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "test_ds = []\n",
    "for i in range(len(X_valid)):\n",
    "    test_ds.append([np.transpose(X_test[i][:,features]), y_test[i]])\n",
    "    \n",
    "    \n",
    "train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)\n",
    "test_ldr = torch.utils.data.DataLoader(test_ds,batch_size=bat_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d37a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "###    CNN+RNN (thesis)     ###\n",
    "###############################\n",
    "\n",
    "if cross_validation == False:\n",
    "\n",
    "    #-------- Train --------#\n",
    "    nsamples, nx, ny = X_train.shape\n",
    "    print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "    nsamples, nx, ny = X_valid.shape\n",
    "    print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "    # Dataloader\n",
    "    train_ds = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "    val_ds = []\n",
    "    for i in range(len(X_valid)):\n",
    "        val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "    train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=len(train_ds), shuffle=True)\n",
    "    val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)\n",
    "\n",
    "    # Initialize network\n",
    "    net = Net_project_simple_CNN_RNN(num_classes=num_classes, \n",
    "             n_features=n_features, \n",
    "             numHN=numHN, \n",
    "             numFilter=numFilter,\n",
    "             dropOutRate=dropOutRate).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate,\n",
    "                           weight_decay=0.0005,\n",
    "                           amsgrad=True,)\n",
    "    \n",
    "    train_acc, train_losses, train_auc, valid_acc, valid_losses, valid_auc, val_preds, val_targs = func.train_project(net, optimizer, train_ldr, val_ldr, test_ldr, X_valid, epochs, criterion, early_stop)\n",
    "\n",
    "\n",
    "    #-------- Performance --------#\n",
    "    epoch = np.arange(1,len(train_losses)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_losses, 'r', epoch, valid_losses, 'b')\n",
    "    plt.legend(['Train Loss','Validation Loss'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "\n",
    "    epoch = np.arange(1,len(train_auc)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_auc, 'r', epoch, valid_auc, 'b')\n",
    "    plt.legend(['Train AUC','Validation AUC'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('AUC')\n",
    "\n",
    "    epoch = np.arange(1,len(train_acc)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "    plt.legend(['Train Accuracy','Validation Accuracy'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('Acc')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    #-------- Save results --------#\n",
    "\n",
    "    results = pd.DataFrame(list(zip( (int(x) for x in val_targs), (int(x) for x in val_preds))),columns =['target', 'pred'])\n",
    "    print(results)\n",
    "\n",
    "    #results.to_csv('results/df_targets_preds_th.csv'.format(str(date.today())), index=False)\n",
    "    \n",
    "    \n",
    "    #-------- Performance Evaluation --------#\n",
    "    # The results change every time we train, we should check why (maybe we missed something or did wrong with the seeds?)\n",
    "\n",
    "    print(\"AUC: \", roc_auc_score(results['target'], results['pred']))\n",
    "    print(\"MCC: \", matthews_corrcoef(results['target'], results['pred']))\n",
    "\n",
    "    confusion_matrix = pd.crosstab(results['target'], results['pred'], rownames=['Actual'], colnames=['Predicted'])\n",
    "    sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot roc curve\n",
    "\n",
    "    fpr, tpr, thres = roc_curve(results['target'], results['pred'])\n",
    "    print('AUC: {:.3f}'.format(roc_auc_score(results['target'], results['pred'])))\n",
    "\n",
    "    print( len([i for i, (a, b) in enumerate(zip(results['pred'], results['target'])) if a != b]))\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    # roc curve\n",
    "    plt.plot(fpr, tpr, \"b\", label='ROC Curve')\n",
    "    plt.plot([0,1],[0,1], \"k--\", label='Random Guess')\n",
    "    plt.xlabel(\"false positive rate\")\n",
    "    plt.ylabel(\"true positive rate\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"ROC curve\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    AUC = roc_auc_score(results['target'], results['pred'])\n",
    "    MCC = matthews_corrcoef(results['target'], results['pred'])\n",
    "    print(\"AUC: \", AUC)\n",
    "    print(\"MCC: \", MCC)\n",
    "    print(\"early stop:\", early_stop)\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing values\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('embedding', embedding) \n",
    "    mlflow.log_param('Hidden Neurons', numHN)\n",
    "    mlflow.log_param('filters CNN', numFilter)\n",
    "    mlflow.log_param('Dropout rate', dropOutRate)\n",
    "    mlflow.log_metric('AUC', AUC)\n",
    "    mlflow.log_metric('MCC', MCC)\n",
    "    #ADD ARTIFACTS (PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebcc79-4feb-4f4f-bf49-0271c1dcacac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e764bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2802b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
