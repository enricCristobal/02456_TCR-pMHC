{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a01d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Libraries --------#\n",
    "import torch\n",
    "#import esm\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import gc\n",
    "#import mlflow\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3423c37a-4b59-4a1f-93f8-4ef70f490869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Modules from project--------#\n",
    "#import encoding as enc\n",
    "from model import Net, Net_thesis, Net_project, Net_project_simple_CNN_RNN, Net_project_transformer_CNN_RNN\n",
    "import functions as func\n",
    "\n",
    "### !!!!!!!!  I need to send new functions and encodings files to you, but for now:  \n",
    "def esm_1b_peptide(peptides, pooling=True):\n",
    "    embeddings = list()\n",
    "    # Load pre-trained ESM-1b model\n",
    "\n",
    "    model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    data = []\n",
    "    seq_rep = []   ### NEW\n",
    "    count = 0\n",
    "    for peptide in peptides:\n",
    "        data.append((\"\", peptide))\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[12], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][12]\n",
    "    del results, batch_labels, batch_strs, batch_tokens, model, alphabet, batch_converter\n",
    "    gc.collect()\n",
    "    print(\"results are ready\")\n",
    "    for i, (_, seq) in enumerate(data):\n",
    "        if pooling:\n",
    "             seq_rep.append( token_representations[i, 1: len(seq) + 1].mean(0) )   ### NEW\n",
    "        else:\n",
    "            seq_rep.append( token_representations[i, 1: len(seq) + 1] )   ### NEW\n",
    "    del token_representations\n",
    "    gc.collect()  \n",
    "    return seq_rep ### NEW\n",
    "\n",
    "def reverseOneHot(encoding):\n",
    "    \"\"\"\n",
    "    Converts one-hot encoded array back to string sequence\n",
    "    \"\"\"\n",
    "    seq=''\n",
    "    for i in range(len(encoding)):\n",
    "            if return_aa(encoding[i].tolist()) != 'X':\n",
    "                seq+=return_aa(encoding[i].tolist())\n",
    "    return seq\n",
    "\n",
    "def extract_sequences(dataset_X, merge=False):\n",
    "    \"\"\"\n",
    "    Return DataFrame with MHC, peptide and TCR a/b sequences from\n",
    "    one-hot encoded complex sequences in dataset X\n",
    "    \"\"\"\n",
    "    mhc_sequences = [reverseOneHot(arr[0:179,0:20]) for arr in dataset_X]\n",
    "    pep_sequences = [reverseOneHot(arr[179:192,0:20]) for arr in dataset_X] ## 190 or 192 ????\n",
    "    tcr_sequences = [reverseOneHot(arr[192:,0:20]) for arr in dataset_X]\n",
    "    all_sequences = [reverseOneHot(arr[:,0:20]) for arr in dataset_X]\n",
    "\n",
    "    if merge:\n",
    "        df_sequences = pd.DataFrame({\"all\": all_sequences})\n",
    "\n",
    "    else:\n",
    "        df_sequences = pd.DataFrame({\"MHC\":mhc_sequences,\n",
    "                                 \"peptide\":pep_sequences,\n",
    "                                 \"TCR\":tcr_sequences})\n",
    "        \n",
    "    return df_sequences        \n",
    "    \n",
    "def extract_sequences(dataset_X, merge=False):\n",
    "    \"\"\"\n",
    "    Return DataFrame with MHC, peptide and TCR a/b sequences from\n",
    "    one-hot encoded complex sequences in dataset X\n",
    "    \"\"\"\n",
    "    mhc_sequences = [reverseOneHot(arr[0:179,0:20]) for arr in dataset_X]\n",
    "    pep_sequences = [reverseOneHot(arr[179:192,0:20]) for arr in dataset_X] ## 190 or 192 ????\n",
    "    tcr_sequences = [reverseOneHot(arr[192:,0:20]) for arr in dataset_X]\n",
    "    all_sequences = [reverseOneHot(arr[:,0:20]) for arr in dataset_X]\n",
    "\n",
    "    if merge:\n",
    "        df_sequences = pd.DataFrame({\"all\": all_sequences})\n",
    "\n",
    "    else:\n",
    "        df_sequences = pd.DataFrame({\"MHC\":mhc_sequences,\n",
    "                                 \"peptide\":pep_sequences,\n",
    "                                 \"TCR\":tcr_sequences})\n",
    "        \n",
    "    return df_sequences\n",
    "    \n",
    "\n",
    "def extract_aa_and_energy_terms(dataset_X):\n",
    "    new_dataset_X = list()\n",
    "    for cmplx in range(len(dataset_X)):\n",
    "        df = pd.DataFrame(dataset_X[cmplx])\n",
    "        df['aa'] = 'A'\n",
    "        for i in range(len(df)):\n",
    "            df['aa'][i] = return_aa(list(df.iloc[i,0:20]))\n",
    "        df = df.iloc[:,20:]\n",
    "        new_dataset_X.append(np.array(df))\n",
    "    return np.array(new_dataset_X)   \n",
    "\n",
    "def return_aa(one_hot):\n",
    "    mapping = dict(zip(range(20),\"ACDEFGHIKLMNPQRSTVWY\"))\n",
    "    try:\n",
    "        index = one_hot.index(1)\n",
    "        return mapping[index]     \n",
    "    except:\n",
    "        return 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4211d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs available. Using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "#-------- Set Device --------#\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPUs available. Using CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5adc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Seeds --------#\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f33ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Directories --------#\n",
    "\n",
    "DATADIR = '/data/'\n",
    "TRAINDIR = '../data/train'\n",
    "VALIDATIONDIR = '../data/validation'\n",
    "MATRICES = '/data/Matrices'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f45575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train already unzipped.\n",
      "../data/validation already unzipped.\n",
      "Train directory:\n",
      "\n",
      " P2_labels.npz\n",
      "P3_input.npz\n",
      "P4_input.npz\n",
      "P2_input.npz\n",
      "__MACOSX\n",
      "P1_input.npz\n",
      "P3_labels.npz\n",
      "P4_labels.npz\n",
      "P1_labels.npz \n",
      "\n",
      "\n",
      "Validation directory:\n",
      "\n",
      " P5_input.npz\n",
      "P5_labels.npz\n",
      "__MACOSX\n"
     ]
    }
   ],
   "source": [
    "#-------- Unzip Train --------#\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(TRAINDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(TRAINDIR))\n",
    "except:\n",
    "    !unzip ../data/train.zip -d ../data/train\n",
    "\n",
    "    \n",
    "#-------- Unzip Validation --------#\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(VALIDATIONDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(VALIDATIONDIR))\n",
    "except:\n",
    "    !unzip ../data/validation.zip -d ../data/validation\n",
    "    \n",
    "print('Train directory:\\n\\n', '\\n'.join(str(p) for p in os.listdir(TRAINDIR)), '\\n\\n')\n",
    "print('Validation directory:\\n\\n','\\n'.join(str(p) for p in os.listdir(VALIDATIONDIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b37f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 5\n",
      "Size of file 0 1480\n",
      "Size of file 1 1532\n",
      "Size of file 2 1168\n",
      "Size of file 3 1526\n",
      "Size of file 4 1207\n"
     ]
    }
   ],
   "source": [
    "#-------- Import Dataset --------#\n",
    "\n",
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "import glob\n",
    "for fp in glob.glob(\"../data/train/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "for fp in glob.glob(\"../data/validation/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "data_partitions = len(data_list)\n",
    "\n",
    "print(\"Number of files:\", data_partitions)\n",
    "\n",
    "for i in range(data_partitions):\n",
    "    print(\"Size of file\", i, len(data_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c65246ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 0\n",
      "{1.0: 370, 0.0: 1110} \n",
      "\n",
      "File: 1\n",
      "{1.0: 383, 0.0: 1149} \n",
      "\n",
      "File: 2\n",
      "{1.0: 292, 0.0: 876} \n",
      "\n",
      "File: 3\n",
      "{1.0: 380, 0.0: 1146} \n",
      "\n",
      "File: 4\n",
      "{1.0: 301, 0.0: 906} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(target_list)):\n",
    "    print(\"File:\", i)\n",
    "    frequency = collections.Counter(target_list[i])\n",
    "    print(dict(frequency), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6876251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation = False\n",
    "merge = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9897fc5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#-------- Hyperparameters to fine tune -------#\n",
    "numHN = 32\n",
    "numFilter = 100\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "\n",
    "dropOutRate = 0.1\n",
    "keep_energy=True\n",
    "embedding = \"baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0610b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed hyperparameters\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "bat_size = 128\n",
    "epochs = 100\n",
    "patience = epochs // 10\n",
    "kernel_size = 3\n",
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be048917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n"
     ]
    }
   ],
   "source": [
    "#-------- Encode ALL --------#\n",
    "\n",
    "if \"embedding\" == 'esm-1b':\n",
    "    \n",
    "    if merge == True:\n",
    "    \n",
    "        start = time.time()\n",
    "\n",
    "        count=0\n",
    "\n",
    "        embedding_dir= '../data/embeddedFiles/'\n",
    "\n",
    "        data_list_new = list()\n",
    "\n",
    "        batch = 5 #### DEBUGGED\n",
    "\n",
    "        print(\"batch:\", batch)\n",
    "\n",
    "        for dataset in data_list[:-1]: ## last file\n",
    "\n",
    "            dataset = dataset\n",
    "            count += 1\n",
    "\n",
    "            print(\"\\nWorking on file\", count, \"- size:\", len(dataset))\n",
    "            encoded = list()\n",
    "\n",
    "            print(\"Merge true\")\n",
    "            x_enc = extract_sequences(dataset, merge=True)\n",
    "            print(\"Sequences are extracted\")\n",
    "            print(\"Start encoding\\n\")\n",
    "            for i in range(0, len( x_enc['all'].tolist() ), batch):\n",
    "                if i % batch == 0:\n",
    "                    print(\"Flag:\", i, \"peptides are encoded - Time:\", round(time.time()-start,2))\n",
    "                all_i = esm_1b_peptide( x_enc['all'].tolist()[i : i + batch] , pooling=False) #### DEBUGGED (batch = any number you wanna pass at once, that your pc can handle)\n",
    "                encoded.extend(all_i) \n",
    "\n",
    "            data_list_new.append(encoded)\n",
    "\n",
    "            print(\"Encoding is done - encoded size:\", len(encoded))\n",
    "            print(\"Save in file\")\n",
    "            outfile = open(embedding_dir + 'dataset-all-{}-file{}'.format(\"esm1b\", count), 'wb')\n",
    "            pickle.dump(encoded, outfile)\n",
    "            outfile.close()\n",
    "            print(\"Saving is done.\\n\\n\")\n",
    "else:\n",
    "    print('baseline')\n",
    "    data_list_enc = data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4642913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nif embedding == \"esm-1b\":\\n    if merge == True:\\n        data_list_enc = list()\\n\\n        for t in range(len(data_list)):\\n            print(t)\\n            data_list_t = list()\\n            for cmplx in range(len(data_list[t])):\\n\\n                df = pd.DataFrame( data_list[t][cmplx] ) #### baseline data to extract enery \\n                x_df = pd.DataFrame( extract_aa_and_energy_terms(data_list[t])[cmplx] ) #### baseline data to find all 0s (paddings) \\n                embedding_complex = pd.DataFrame(data_list_new[t][cmplx]) ##### new embeded data\\n\\n                pad_index_list = sorted(x_df[x_df.iloc[:,34]==\\'X\\'].index.tolist())\\n\\n                padding1_len = 0\\n                for pad in range(len(pad_index_list)):\\n                    padding1_len += 1\\n                    if pad_index_list[pad+1]-pad_index_list[pad] > 100:\\n                        tcr_start = pad_index_list[pad] + 1\\n                        tcr_end = pad_index_list[pad+1]\\n                        break\\n                padding2_len = len(pad_index_list) - padding1_len\\n\\n                mhc = pd.concat([df.iloc[:179,20:].reset_index(drop=True), ///embedding_complex[??] here put mhc part of your embedding -> 0:179/// ], axis=1).iloc[:, :-1].reset_index(drop=True)\\n                pep = pd.concat([df.iloc[179:188,20:].reset_index(drop=True), ///embedding_complex[??] peptide part of your embedding -> 179:179+9/// ], axis=1).iloc[:, :-1].reset_index(drop=True)\\n                tcr = pd.concat([(df.iloc[tcr_start:tcr_end,20:]).reset_index(drop=True), ///embedding_complex[??] tcr partofyourembedding -> tcrstart: tcrend/// ], axis=1).iloc[:, :-1].reset_index(drop=True)\\n                padding = pd.DataFrame(0, index=np.arange(padding1_len + padding2_len), columns=pep.columns)\\n\\n                final_cmplx = pd.concat([mhc,pep,tcr,padding]).reset_index(drop=True)\\n                final_cmplx.columns = np.arange(len(final_cmplx.columns))\\n                data_list_t.append(np.array(final_cmplx))\\n\\n            data_list_enc.append(np.array(data_list_t))\\n            \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------- Paste Energy Terms Next to it --------#  ########### !!! May require debugging\n",
    "'''\n",
    "if embedding == \"esm-1b\":\n",
    "    if merge == True:\n",
    "        data_list_enc = list()\n",
    "\n",
    "        for t in range(len(data_list)):\n",
    "            print(t)\n",
    "            data_list_t = list()\n",
    "            for cmplx in range(len(data_list[t])):\n",
    "\n",
    "                df = pd.DataFrame( data_list[t][cmplx] ) #### baseline data to extract enery \n",
    "                x_df = pd.DataFrame( extract_aa_and_energy_terms(data_list[t])[cmplx] ) #### baseline data to find all 0s (paddings) \n",
    "                embedding_complex = pd.DataFrame(data_list_new[t][cmplx]) ##### new embeded data\n",
    "\n",
    "                pad_index_list = sorted(x_df[x_df.iloc[:,34]=='X'].index.tolist())\n",
    "\n",
    "                padding1_len = 0\n",
    "                for pad in range(len(pad_index_list)):\n",
    "                    padding1_len += 1\n",
    "                    if pad_index_list[pad+1]-pad_index_list[pad] > 100:\n",
    "                        tcr_start = pad_index_list[pad] + 1\n",
    "                        tcr_end = pad_index_list[pad+1]\n",
    "                        break\n",
    "                padding2_len = len(pad_index_list) - padding1_len\n",
    "\n",
    "                mhc = pd.concat([df.iloc[:179,20:].reset_index(drop=True), ///embedding_complex[??] here put mhc part of your embedding -> 0:179/// ], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "                pep = pd.concat([df.iloc[179:188,20:].reset_index(drop=True), ///embedding_complex[??] peptide part of your embedding -> 179:179+9/// ], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "                tcr = pd.concat([(df.iloc[tcr_start:tcr_end,20:]).reset_index(drop=True), ///embedding_complex[??] tcr partofyourembedding -> tcrstart: tcrend/// ], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "                padding = pd.DataFrame(0, index=np.arange(padding1_len + padding2_len), columns=pep.columns)\n",
    "\n",
    "                final_cmplx = pd.concat([mhc,pep,tcr,padding]).reset_index(drop=True)\n",
    "                final_cmplx.columns = np.arange(len(final_cmplx.columns))\n",
    "                data_list_t.append(np.array(final_cmplx))\n",
    "\n",
    "            data_list_enc.append(np.array(data_list_t))\n",
    "            \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53292ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"esm-1b\":\n",
    "    print(final_cmplx.to_markdown()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e438631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "5 \n",
      "\n",
      "number of complexes: 1480\n",
      "number of rows: 420\n",
      "number of columns: 54\n",
      "\n",
      "\n",
      "number of complexes: 1532\n",
      "number of rows: 420\n",
      "number of columns: 54\n",
      "\n",
      "\n",
      "number of complexes: 1168\n",
      "number of rows: 420\n",
      "number of columns: 54\n",
      "\n",
      "\n",
      "number of complexes: 1526\n",
      "number of rows: 420\n",
      "number of columns: 54\n",
      "\n",
      "\n",
      "number of complexes: 1207\n",
      "number of rows: 420\n",
      "number of columns: 54\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if embedding == \"baseline\":\n",
    "    print(\"baseline\")\n",
    "    print(len(data_list_enc), \"\\n\")\n",
    "\n",
    "    for i in range(len(data_list)):\n",
    "        print(\"number of complexes:\", len(data_list[i]))\n",
    "        print(\"number of rows:\", len(data_list[i][0]))\n",
    "        print(\"number of columns:\", len(data_list[i][0][0]))\n",
    "        print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"ESM 1B:\")\n",
    "    final_cmplx\n",
    "\n",
    "    print(len(data_list_enc), \"\\n\")\n",
    "\n",
    "    for i in range(len(data_list_enc)):\n",
    "        print(\"number of complexes:\", len(data_list_enc[i]))\n",
    "        print(\"number of rows:\", len(data_list_enc[i][0]))\n",
    "        print(\"number of columns:\", len(data_list_enc[i][0][0]))\n",
    "        print(\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f86cba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: 4180 420 54\n",
      "Validation set shape: 1526 420 54\n",
      "Test set shape: 1207 420 54\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate(data_list_enc[0:3])\n",
    "y_train = np.concatenate(target_list[0:3])\n",
    "nsamples, nx, ny = X_train.shape\n",
    "print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "X_valid = np.concatenate(data_list_enc[3:4])\n",
    "y_valid = np.concatenate(target_list[3:4])\n",
    "nsamples, nx, ny = X_valid.shape\n",
    "print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "X_test = np.concatenate(data_list_enc[4:5])\n",
    "y_test = np.concatenate(target_list[4:5])\n",
    "nsamples, nx, ny = X_test.shape\n",
    "print(\"Test set shape:\", nsamples,nx,ny)\n",
    "\n",
    "# features and residues\n",
    "features = list(range(ny))\n",
    "residues = list(range(nx)) \n",
    "n_features = len(features)\n",
    "input_size = len(residues)\n",
    "\n",
    "# Dataloader\n",
    "train_ds = []\n",
    "for i in range(len(X_train)):\n",
    "    train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "    \n",
    "val_ds = []\n",
    "for i in range(len(X_valid)):\n",
    "    val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "    \n",
    "test_ds = []\n",
    "for i in range(len(X_test)):\n",
    "    test_ds.append([np.transpose(X_test[i][:,features]), y_test[i]])\n",
    "    \n",
    "    \n",
    "train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)\n",
    "test_ldr = torch.utils.data.DataLoader(test_ds,batch_size=len(test_ds), shuffle=True) ## Test at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153ff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "cross_validation False\n",
      "merge True\n",
      "embedding baseline\n",
      "numHN 32\n",
      "numFilter 100\n",
      "dropOutRate 0.1\n",
      "keep_energy True\n",
      "num_classes 1\n",
      "learning_rate 0.001\n",
      "bat_size 128\n",
      "patience 10\n",
      "criterion BCEWithLogitsLoss()\n",
      "weight_decay 0.0005\n",
      "\n",
      "\n",
      "\n",
      "Train\n",
      "Epoch 0  \t Train loss: 0.00455 \t Validation loss: 0.00419\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "###    CNN+RNN (thesis)     ###\n",
    "###############################\n",
    "start = time.time()\n",
    "\n",
    "if cross_validation == False:\n",
    "    \n",
    "    print(\"Parameters:\")\n",
    "    print(\"cross_validation\", cross_validation)\n",
    "    print(\"merge\", merge)\n",
    "    print(\"embedding\", embedding)\n",
    "    print(\"numHN\", numHN)\n",
    "    print(\"numFilter\", numFilter)\n",
    "    print(\"dropOutRate\", dropOutRate)\n",
    "    print(\"keep_energy\", keep_energy)\n",
    "    print(\"num_classes\", num_classes)\n",
    "    print(\"learning_rate\", learning_rate)\n",
    "    print(\"bat_size\", bat_size)\n",
    "    print(\"patience\", patience)\n",
    "    print(\"criterion\", criterion)\n",
    "    print(\"weight_decay\", 0.0005)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    #-------- Train --------#\n",
    "\n",
    "    print(\"Train\")\n",
    "    \n",
    "    # Initialize network\n",
    "    net = Net_project_simple_CNN_RNN(num_classes = num_classes, \n",
    "             n_features = n_features, \n",
    "             numHN_lstm = numHN,\n",
    "             numFilter = numFilter,\n",
    "             dropOutRate = 0.1)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate,\n",
    "                           weight_decay=0.0005,\n",
    "                           amsgrad=True,)\n",
    "    \n",
    "    train_acc, train_losses, train_auc, valid_acc, valid_losses, valid_auc, val_preds, val_targs, test_preds, test_targs, test_loss, test_acc, test_auc = func.train_project(net, optimizer, train_ldr, val_ldr, test_ldr, X_valid, epochs, criterion, patience)\n",
    "\n",
    "else:\n",
    "    pass\n",
    "\n",
    "print(\"Done in\", round((time.time()-start)/60,2), \"mins.\" )\n",
    "\n",
    "print(\"test_acc, test_auc:\")\n",
    "print(test_acc, \",\", test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5bdb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len([x.item() for x in train_losses]))\n",
    "print(len([x.item() for x in valid_losses]))\n",
    "print(len(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c61551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Performance --------#\n",
    "\n",
    "epoch = np.arange(1,len(train_losses)+1)\n",
    "plt.figure()\n",
    "plt.plot(epoch, [x.item() for x in train_losses], 'r', epoch, [x.item() for x in valid_losses], 'b')\n",
    "plt.legend(['Train Loss','Validation Loss'])\n",
    "plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "\n",
    "epoch = np.arange(1,len(train_auc)+1)\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_auc, 'r', epoch, valid_auc, 'b')\n",
    "plt.legend(['Train AUC','Validation AUC'])\n",
    "plt.xlabel('Epoch'), plt.ylabel('AUC')\n",
    "\n",
    "epoch = np.arange(1,len(train_acc)+1)\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "plt.legend(['Train Accuracy','Validation Accuracy'])\n",
    "plt.xlabel('Epoch'), plt.ylabel('Acc')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#-------- Save results --------#\n",
    "\n",
    "results = pd.DataFrame(list(zip( (x[0] for x in test_targs), (x[0] for x in test_preds))),columns =['target', 'pred'])\n",
    "#print(results)\n",
    "\n",
    "#results.to_csv('results/df_targets_preds_th.csv'.format(str(date.today())), index=False)\n",
    "\n",
    "\n",
    "#-------- Performance Evaluation --------#\n",
    "# The results change every time we train, we should check why (maybe we missed something or did wrong with the seeds?)\n",
    "\n",
    "print(\"AUC: \", roc_auc_score(results['target'], results['pred']))\n",
    "print(\"MCC: \", matthews_corrcoef(results['target'], results['pred']))\n",
    "\n",
    "confusion_matrix = pd.crosstab(results['target'], results['pred'], rownames=['Actual'], colnames=['Predicted'])\n",
    "sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.show()\n",
    "\n",
    "# Plot roc curve\n",
    "\n",
    "fpr, tpr, thres = roc_curve(results['target'], results['pred'])\n",
    "print('AUC: {:.3f}'.format(roc_auc_score(results['target'], results['pred'])))\n",
    "\n",
    "print( len([i for i, (a, b) in enumerate(zip(results['pred'], results['target'])) if a != b]))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# roc curve\n",
    "plt.plot(fpr, tpr, \"b\", label='ROC Curve')\n",
    "plt.plot([0,1],[0,1], \"k--\", label='Random Guess')\n",
    "plt.xlabel(\"false positive rate\")\n",
    "plt.ylabel(\"true positive rate\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.title(\"ROC curve\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "AUC = roc_auc_score(results['target'], results['pred'])\n",
    "MCC = matthews_corrcoef(results['target'], results['pred'])\n",
    "ACC = accuracy_score(results['target'], results['pred'])\n",
    "print(\"AUC: \", AUC)\n",
    "print(\"ACC: \", ACC)\n",
    "print(\"MCC: \", MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#storing values\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('embedding', embedding) \n",
    "    mlflow.log_param('Hidden Neurons', numHN)\n",
    "    mlflow.log_param('filters CNN', numFilter)\n",
    "    mlflow.log_param('Dropout rate', dropOutRate)\n",
    "    mlflow.log_metric('AUC', AUC)\n",
    "    mlflow.log_metric('MCC', MCC)\n",
    "    #ADD ARTIFACTS (PLOTS)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebcc79-4feb-4f4f-bf49-0271c1dcacac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e764bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2802b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
