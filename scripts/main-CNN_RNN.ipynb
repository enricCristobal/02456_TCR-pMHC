{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a01d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Libraries --------#\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import mlflow\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3423c37a-4b59-4a1f-93f8-4ef70f490869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Modules from project--------#\n",
    "import encoding as enc\n",
    "from model import Net, Net_thesis, Net_project, Net_project_simple_CNN_RNN, Net_project_transformer_CNN_RNN\n",
    "import functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4211d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs available. Using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "#-------- Set Device --------#\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPUs available. Using CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5adc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Seeds --------#\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f33ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Directories --------#\n",
    "\n",
    "DATADIR = '/data/'\n",
    "TRAINDIR = '../data/train'\n",
    "VALIDATIONDIR = '../data/validation'\n",
    "MATRICES = '/data/Matrices'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f45575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train already unzipped.\n",
      "../data/validation already unzipped.\n",
      "Train directory:\n",
      "\n",
      " P2_labels.npz\n",
      "P3_input.npz\n",
      "P4_input.npz\n",
      "P2_input.npz\n",
      "__MACOSX\n",
      "P1_input.npz\n",
      "P3_labels.npz\n",
      "P4_labels.npz\n",
      "P1_labels.npz \n",
      "\n",
      "\n",
      "Validation directory:\n",
      "\n",
      " P5_input.npz\n",
      "P5_labels.npz\n",
      "__MACOSX\n"
     ]
    }
   ],
   "source": [
    "#-------- Unzip Train --------#\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(TRAINDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(TRAINDIR))\n",
    "except:\n",
    "    !unzip ../data/train.zip -d ../data/train\n",
    "\n",
    "    \n",
    "#-------- Unzip Validation --------#\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(VALIDATIONDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(VALIDATIONDIR))\n",
    "except:\n",
    "    !unzip ../data/validation.zip -d ../data/validation\n",
    "    \n",
    "print('Train directory:\\n\\n', '\\n'.join(str(p) for p in os.listdir(TRAINDIR)), '\\n\\n')\n",
    "print('Validation directory:\\n\\n','\\n'.join(str(p) for p in os.listdir(VALIDATIONDIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b37f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 5\n",
      "Size of file 0 1480\n",
      "Size of file 1 1532\n",
      "Size of file 2 1168\n",
      "Size of file 3 1526\n",
      "Size of file 4 1207\n"
     ]
    }
   ],
   "source": [
    "#-------- Import Dataset --------#\n",
    "\n",
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "import glob\n",
    "for fp in glob.glob(\"../data/train/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "for fp in glob.glob(\"../data/validation/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "data_partitions = len(data_list)\n",
    "\n",
    "print(\"Number of files:\", data_partitions)\n",
    "\n",
    "for i in range(data_partitions):\n",
    "    print(\"Size of file\", i, len(data_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c65246ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 0\n",
      "{1.0: 370, 0.0: 1110} \n",
      "\n",
      "File: 1\n",
      "{1.0: 383, 0.0: 1149} \n",
      "\n",
      "File: 2\n",
      "{1.0: 292, 0.0: 876} \n",
      "\n",
      "File: 3\n",
      "{1.0: 380, 0.0: 1146} \n",
      "\n",
      "File: 4\n",
      "{1.0: 301, 0.0: 906} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(target_list)):\n",
    "    print(\"File:\", i)\n",
    "    frequency = collections.Counter(target_list[i])\n",
    "    print(dict(frequency), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6876251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation = False\n",
    "merge = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9897fc5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#-------- Hyperparameters to fine tune -------#\n",
    "embedding = \"esm-1b\" #baseline\n",
    "numHN=64\n",
    "numFilter=100\n",
    "dropOutRate=0.1\n",
    "keep_energy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be048917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 \n",
      "\n",
      "File 0\n",
      "MHC: ['GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETL']\n",
      "MHC length range: [179]\n",
      "PEP length range: [9]\n",
      "TCR length range: [200, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 228]\n",
      "\n",
      "\n",
      "File 1\n",
      "MHC: ['GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETL']\n",
      "MHC length range: [179]\n",
      "PEP length range: [9]\n",
      "TCR length range: [201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227]\n",
      "\n",
      "\n",
      "File 2\n",
      "MHC: ['GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETL']\n",
      "MHC length range: [179]\n",
      "PEP length range: [9]\n",
      "TCR length range: [202, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]\n",
      "\n",
      "\n",
      "File 3\n",
      "MHC: ['GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETL']\n",
      "MHC length range: [179]\n",
      "PEP length range: [9]\n",
      "TCR length range: [200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]\n",
      "\n",
      "\n",
      "File 4\n",
      "MHC: ['GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETL']\n",
      "MHC length range: [179]\n",
      "PEP length range: [9]\n",
      "TCR length range: [202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(data_list), \"\\n\")\n",
    "\n",
    "for i in range(len(data_list)):\n",
    "    print(\"File\", i)\n",
    "    seq = func.extract_sequences(data_list[i])\n",
    "\n",
    "    seq['PEP_len'] = seq['peptide'].str.len()\n",
    "    seq['TCR_len'] = seq['TCR'].str.len()\n",
    "    seq['MHC_len'] = seq['MHC'].str.len()\n",
    "\n",
    "    print(\"MHC:\", list(set(seq['MHC'].tolist())))\n",
    "    leng_TCR = list()\n",
    "    leng_MHC = list()\n",
    "    leng_PEP = list()\n",
    "\n",
    "    for i in range(len(seq)):\n",
    "\n",
    "        if seq['PEP_len'][i] not in leng_PEP:\n",
    "            leng_PEP.append(seq['PEP_len'][i])\n",
    "\n",
    "        if seq['MHC_len'][i] not in leng_MHC:\n",
    "            leng_MHC.append(seq['MHC_len'][i]) \n",
    "\n",
    "        if seq['TCR_len'][i] not in leng_TCR:\n",
    "            leng_TCR.append(seq['TCR_len'][i])\n",
    "\n",
    "    print(\"MHC length range:\", sorted(leng_MHC))\n",
    "    print(\"PEP length range:\",sorted(leng_PEP))\n",
    "    print(\"TCR length range:\",sorted(leng_TCR))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2506f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_list_aa = list()\n",
    "\n",
    "#for i in range(5):\n",
    "#    data_list_aa.append(func.extract_aa_and_energy_terms(data_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "852a50bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure of 1 complex:\n",
    "#pd.DataFrame(data_list_aa[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c397f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm-1b\n",
      "\n",
      "Working on file 0\n",
      "Sequences are extracted\n",
      "Merge false\n",
      "1480 1480 1480\n",
      "\n",
      "Flag 0  - Time: 23.59\n",
      "\n",
      "Flag 5  - Time: 160.59\n",
      "\n",
      "Flag 10  - Time: 298.12\n",
      "\n",
      "Flag 15  - Time: 436.27\n",
      "\n",
      "Flag 20  - Time: 575.95\n",
      "\n",
      "Flag 25  - Time: 724.51\n",
      "\n",
      "Flag 30  - Time: 872.56\n",
      "\n",
      "Flag 35  - Time: 1018.06\n",
      "\n",
      "Flag 40  - Time: 1165.62\n",
      "\n",
      "Flag 45  - Time: 1314.49\n",
      "\n",
      "Flag 50  - Time: 1459.41\n",
      "\n",
      "Flag 55  - Time: 1606.41\n",
      "\n",
      "Flag 60  - Time: 1754.92\n",
      "\n",
      "Flag 65  - Time: 1904.53\n",
      "\n",
      "Flag 70  - Time: 2054.34\n",
      "\n",
      "Flag 75  - Time: 2200.7\n",
      "\n",
      "Flag 80  - Time: 2348.77\n",
      "\n",
      "Flag 85  - Time: 2503.43\n",
      "\n",
      "Flag 90  - Time: 2658.84\n",
      "\n",
      "Flag 95  - Time: 2824.47\n",
      "\n",
      "Flag 100  - Time: 2983.21\n",
      "\n",
      "Flag 105  - Time: 3152.48\n",
      "\n",
      "Flag 110  - Time: 3316.94\n",
      "\n",
      "Flag 115  - Time: 3480.01\n",
      "\n",
      "Flag 120  - Time: 3635.1\n",
      "\n",
      "Flag 125  - Time: 3792.09\n",
      "\n",
      "Flag 130  - Time: 3969.57\n"
     ]
    }
   ],
   "source": [
    "#embedding of data\n",
    "data_list_enc_mhc = list()\n",
    "data_list_enc_pep = list() \n",
    "data_list_enc_tcr = list()\n",
    "start = time.time()\n",
    "mhc_bool = False\n",
    "\n",
    "#create directory to fetch/store embedded\n",
    "embedding_dir= '../data/embeddedFiles/'\n",
    "\n",
    "try:\n",
    "    os.mkdir(embedding_dir)\n",
    "    data_list_enc = []\n",
    "    if embedding == \"baseline\":\n",
    "        print('baseline')\n",
    "        data_list_enc = data_list\n",
    "    \n",
    "    elif embedding == \"esm-1b\":\n",
    "        print(\"esm-1b\")\n",
    "        count = 0\n",
    "        \n",
    "        for dataset in data_list:\n",
    "            \n",
    "            mhc_enc = list()\n",
    "            pep_enc = list()\n",
    "            tcr_enc = list()\n",
    "            \n",
    "            print(\"\\nWorking on file\", count)\n",
    "            count += 1\n",
    "            x_enc = func.extract_sequences(dataset, merge=merge)\n",
    "            print(\"Sequences are extracted\")\n",
    "            \n",
    "            if merge:\n",
    "                print(\"Merge true\")\n",
    "                #print(x_enc['all'].tolist())\n",
    "                x_enc = enc.esm_1b(x_enc['all'].tolist(), pooling=False)\n",
    "                x_enc = np.array(x_enc[0])\n",
    "                print(x_enc.shape)\n",
    "                data_list_enc.append(x_enc)\n",
    "                #save\n",
    "                outfile = open(embedding_dir + 'dataset-all-{}-file{}-500'.format(embedding, count),'wb')\n",
    "                pickle.dump(data_list_enc, outfile)\n",
    "                outfile.close()\n",
    "                \n",
    "            else:\n",
    "                print(\"Merge false\")\n",
    "                print(len(x_enc['MHC'].tolist()), len(x_enc['peptide'].tolist()), len(x_enc['TCR'].tolist()))\n",
    "                \n",
    "                if mhc_bool == False:\n",
    "                    mhc_enc_1 = enc.esm_1b_peptide(x_enc['MHC'].tolist()[i], pooling=False)\n",
    "                    mhc_bool == True\n",
    "                    \n",
    "                for i in range(len(x_enc['MHC'].tolist())):\n",
    "                    if i % 5 == 0:\n",
    "                        print(\"\\nFlag\", i, \" - Time:\", round(time.time()-start,2))\n",
    "                    mhc_enc.append(mhc_enc_1)\n",
    "                    pep_enc.append(enc.esm_1b_peptide(x_enc['peptide'].tolist()[i], pooling=False))\n",
    "                    tcr_enc.append(enc.esm_1b_peptide(x_enc['TCR'].tolist()[i], pooling=False))\n",
    "                \n",
    "                mhc_enc = [x.tolist() for x in mhc_enc]\n",
    "                pep_enc = [x.tolist() for x in pep_enc]\n",
    "                tcr_enc = [x.tolist() for x in tcr_enc]\n",
    "                \n",
    "                #print(\"results are stacking\")\n",
    "                #x_enc = np.vstack((mhc_enc[0],pep_enc[0],tcr_enc[0]))\n",
    "                #print(\"before extending:\", x_enc.shape)\n",
    "                \n",
    "                # x_enc should be enc + energy terms\n",
    "                data_list_enc_mhc.append(mhc_enc) \n",
    "                data_list_enc_pep.append(pep_enc) \n",
    "                data_list_enc_tcr.append(tcr_enc) \n",
    "                print(\"ESM_1B is done\\n\")\n",
    "\n",
    "                #save\n",
    "                outfilemhc = open(embedding_dir + 'dataset-{}-file{}-mhc'.format(embedding, count),'wb')\n",
    "                outfilepep = open(embedding_dir + 'dataset-{}-file{}-pep'.format(embedding, count),'wb')\n",
    "                outfiletcr = open(embedding_dir + 'dataset-{}-file{}-tcr'.format(embedding, count),'wb')\n",
    "                pickle.dump(mhc_enc, outfilemhc)\n",
    "                pickle.dump(pep_enc, outfilepep)\n",
    "                pickle.dump(tcr_enc, outfiletcr)\n",
    "                outfilemhc.close()\n",
    "                outfilepep.close()\n",
    "                outfiletcr.close()\n",
    "                \n",
    "\n",
    "    elif embedding == \"esm_ASM\":\n",
    "        for dataset in data_list:\n",
    "            x_enc = func.extract_sequences(dataset, merge=merge).values.tolist()\n",
    "            print(data_list_enc)\n",
    "            x_enc = [enc.esm_ASM(seq, pooling=False) for seq in x_enc]\n",
    "            data_list_enc.append(x_enc)\n",
    "            \n",
    "        #save\n",
    "        outfile = open(embedding_dir + 'dataset-{}-file{}'.format(embedding, count),'wb')\n",
    "        pickle.dump(data_list_enc, outfile)\n",
    "        outfile.close()\n",
    "        \n",
    "    else:         \n",
    "        for dataset in data_list:\n",
    "            x_enc = func.extract_sequences(dataset, merge=merge).values.tolist()\n",
    "            print(data_list_enc)\n",
    "            x_enc = [enc.encodePeptidesCNN(seq, scheme=embedding) for seq in x_enc]\n",
    "            data_list_enc.append(x_enc)\n",
    "        print(data_list_enc)\n",
    "        \n",
    "        #save\n",
    "        outfile = open(embedding_dir + 'dataset-{}-file{}'.format(embedding, count),'wb')\n",
    "        pickle.dump(data_list_enc, outfile)\n",
    "        outfile.close()\n",
    "            \n",
    "except:\n",
    "    for dataset in data_list:\n",
    "        count = -1\n",
    "        count += 1\n",
    "        if merge:\n",
    "            with open(embedding_dir+'dataset-all-{}'.format(embedding)) as f:\n",
    "                data_list_enc =  pickle.load(f)\n",
    "        else:\n",
    "            with open(embedding_dir + 'dataset-{}-file{}-mhc'.format(embedding, count)) as f:\n",
    "                data_list_enc_mhc.append(pickle.load(f))\n",
    "            with open(embedding_dir + 'dataset-{}-file{}-pep'.format(embedding, count)) as f:\n",
    "                data_list_enc_pep.append(pickle.load(f))\n",
    "            with open(embedding_dir + 'dataset-{}-file{}-tcr'.format(embedding, count)) as f:\n",
    "                data_list_enc_tcr.append(pickle.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ccbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f2db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"esm-1b\":\n",
    "    pd.DataFrame(data_list_enc_tcr[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfd05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"esm-1b\":\n",
    "    data_list_enc = list()\n",
    "    for t in range(len(data_list)):\n",
    "        print(t)\n",
    "        data_list_t = list()\n",
    "        for cmplx in range(len(data_list[t])):\n",
    "\n",
    "            df = pd.DataFrame( data_list[t][cmplx] )\n",
    "            new_df = pd.DataFrame( func.extract_aa_and_energy_terms(data_list[t])[cmplx] )\n",
    "\n",
    "            df_emb_mhc = pd.DataFrame(data_list_enc_mhc[t][cmplx])\n",
    "            df_emb_pep = pd.DataFrame(data_list_enc_pep[t][cmplx])\n",
    "            df_emb_tcr = pd.DataFrame(data_list_enc_tcr[t][cmplx])\n",
    "\n",
    "            df_emb_mhc['aa'] = '-'\n",
    "            df_emb_pep['aa'] = '-'    \n",
    "            df_emb_tcr['aa'] = '-'\n",
    "\n",
    "            for mhc in range(len(df_emb_mhc)):\n",
    "                df_emb_mhc['aa'][mhc] = func.return_aa(list(df.iloc[i,0:20]))\n",
    "\n",
    "            for pep in range(len(df_emb_pep)):\n",
    "                df_emb_pep['aa'][pep] = func.return_aa(list(df.iloc[179+i,0:20]))\n",
    "\n",
    "            pad_index_list = sorted(new_df[new_df.iloc[:,34]=='X'].index.tolist())\n",
    "\n",
    "            padding1_len = 0\n",
    "            for pad in range(len(pad_index_list)):\n",
    "                padding1_len += 1\n",
    "                if pad_index_list[pad+1]-pad_index_list[pad] > 100:\n",
    "                    tcr_start = pad_index_list[pad] + 1\n",
    "                    tcr_end = pad_index_list[pad+1]\n",
    "                    break\n",
    "            padding2_len = len(pad_index_list) - padding1_len\n",
    "\n",
    "            for tcr in range(len(df_emb_tcr)):\n",
    "                df_emb_tcr['aa'][tcr] = func.return_aa(list(df.iloc[tcr_start+tcr, 0:20]))\n",
    "\n",
    "            mhc = pd.concat([df_emb_mhc.reset_index(drop=True), df.iloc[:179,20:].reset_index(drop=True)], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "            pep = pd.concat([df_emb_pep.reset_index(drop=True), df.iloc[179:188,20:].reset_index(drop=True)df_emb_pep.reset_index(drop=True), ], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "            padding1 = pd.DataFrame(0, index=np.arange(padding1_len), columns=pep.columns)\n",
    "            tcr = pd.concat([df_emb_tcr.reset_index(drop=True), (df.iloc[tcr_start:tcr_end,20:]).reset_index(drop=True)], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "            padding2 = pd.DataFrame(0, index=np.arange(padding2_len), columns=pep.columns)\n",
    "\n",
    "            final_cmplx = pd.concat([mhc,pep,padding1,tcr,padding2]).reset_index(drop=True)\n",
    "            final_cmplx.columns = np.arange(len(final_cmplx.columns))\n",
    "            data_list_t.append(np.array(final_cmplx))\n",
    "\n",
    "        data_list_enc.append(np.array(data_list_t))\n",
    "\n",
    "Dayana = False\n",
    "if Dayana==True:\n",
    "    if embedding == \"esm-1b\":\n",
    "        data_list_enc = list()\n",
    "        for t in range(len(data_list)):\n",
    "            print(t)\n",
    "            data_list_t = list()\n",
    "            for cmplx in range(len(data_list[t])):\n",
    "\n",
    "                df = pd.DataFrame( data_list[t][cmplx] )\n",
    "                new_df = pd.DataFrame( func.extract_aa_and_energy_terms(data_list[t])[cmplx] ) ## I wrote this func, i can send you\n",
    "\n",
    "                padding1_len = 0\n",
    "                for pad in range(len(pad_index_list)):\n",
    "                    padding1_len += 1\n",
    "                    if pad_index_list[pad+1]-pad_index_list[pad] > 100:\n",
    "                        tcr_start = pad_index_list[pad] + 1\n",
    "                        tcr_end = pad_index_list[pad+1]\n",
    "                        break\n",
    "                padding2_len = len(pad_index_list) - padding1_len\n",
    "\n",
    "\n",
    "                mhc = pd.concat([df.iloc[:179,20:].reset_index(drop=True), ///here put mhc part of your embedding -> 0:179/// ], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "                pep = pd.concat([df.iloc[179:188,20:].reset_index(drop=True), ///peptide part of your embedding -> 179:179+9/// ], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "                tcr = pd.concat([(df.iloc[tcr_start:tcr_end,20:]).reset_index(drop=True), ///tcr partofyourembedding -> tcrstart: tcrend/// ], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "                padding = pd.DataFrame(0, index=np.arange(padding1_len + padding2_len), columns=pep.columns)\n",
    "\n",
    "                final_cmplx = pd.concat([mhc,pep,tcr,padding]).reset_index(drop=True)\n",
    "                final_cmplx.columns = np.arange(len(final_cmplx.columns))\n",
    "                data_list_t.append(np.array(final_cmplx))\n",
    "\n",
    "            data_list_enc.append(np.array(data_list_t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53292ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"esm-1b\":\n",
    "    print(final_cmplx.to_markdown()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e438631",
   "metadata": {},
   "outputs": [],
   "source": [
    "if embedding == \"baseline\":\n",
    "    print(\"baseline\")\n",
    "    print(len(data_list_enc), \"\\n\")\n",
    "\n",
    "    for i in range(len(data_list)):\n",
    "        print(\"number of complexes:\", len(data_list[i]))\n",
    "        print(\"number of rows:\", len(data_list[i][0]))\n",
    "        print(\"number of columns:\", len(data_list[i][0][0]))\n",
    "        print(\"\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"ESM 1B:\")\n",
    "    final_cmplx\n",
    "\n",
    "    print(len(data_list_enc), \"\\n\")\n",
    "\n",
    "    for i in range(len(data_list_enc)):\n",
    "        print(\"number of complexes:\", len(data_list_enc[i]))\n",
    "        print(\"number of rows:\", len(data_list_enc[i][0]))\n",
    "        print(\"number of columns:\", len(data_list_enc[i][0][0]))\n",
    "        print(\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed hyperparameters\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "bat_size = 128\n",
    "epochs = 100\n",
    "riterion = nn.BCEWithLogitsLoss()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate(data_list_enc[0:3])\n",
    "y_train = np.concatenate(target_list[0:3])\n",
    "nsamples, nx, ny = X_train.shape\n",
    "print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "X_valid = np.concatenate(data_list_enc[3:4])\n",
    "y_valid = np.concatenate(target_list[3:4])\n",
    "nsamples, nx, ny = X_valid.shape\n",
    "print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "\n",
    "X_test = np.concatenate(data_list_enc[3:4])\n",
    "y_test = np.concatenate(target_list[3:4])\n",
    "nsamples, nx, ny = X_test.shape\n",
    "print(\"Test set shape:\", nsamples,nx,ny)\n",
    "\n",
    "# features and residues\n",
    "features = list(range(ny))\n",
    "residues = list(range(nx)) \n",
    "n_features = len(features)\n",
    "input_size = len(residues)\n",
    "\n",
    "# Dataloader\n",
    "train_ds = []\n",
    "for i in range(len(X_train)):\n",
    "    train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "val_ds = []\n",
    "for i in range(len(X_valid)):\n",
    "    val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "test_ds = []\n",
    "for i in range(len(X_valid)):\n",
    "    test_ds.append([np.transpose(X_test[i][:,features]), y_test[i]])\n",
    "    \n",
    "    \n",
    "train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)\n",
    "test_ldr = torch.utils.data.DataLoader(test_ds,batch_size=bat_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "###    CNN+RNN (thesis)     ###\n",
    "###############################\n",
    "\n",
    "if cross_validation == False:\n",
    "\n",
    "    #-------- Train --------#\n",
    "    nsamples, nx, ny = X_train.shape\n",
    "    print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "    nsamples, nx, ny = X_valid.shape\n",
    "    print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "    # Dataloader\n",
    "    train_ds = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "    val_ds = []\n",
    "    for i in range(len(X_valid)):\n",
    "        val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "    train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "    val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)\n",
    "\n",
    "    # Initialize network\n",
    "    net = Net_project_simple_CNN_RNN(num_classes=num_classes, \n",
    "             n_features=n_features, \n",
    "             numHN=numHN, \n",
    "             numFilter=numFilter,\n",
    "             dropOutRate=dropOutRate).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate,\n",
    "                           weight_decay=0.0005,\n",
    "                           amsgrad=True,)\n",
    "    \n",
    "    train_acc, train_losses, train_auc, valid_acc, valid_losses, valid_auc, val_preds, val_targs = func.train_project(net, optimizer, train_ldr, val_ldr, test_ldr, X_valid, epochs, criterion, early_stop)\n",
    "\n",
    "\n",
    "    #-------- Performance --------#\n",
    "    epoch = np.arange(1,len(train_losses)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_losses, 'r', epoch, valid_losses, 'b')\n",
    "    plt.legend(['Train Loss','Validation Loss'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "\n",
    "    epoch = np.arange(1,len(train_auc)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_auc, 'r', epoch, valid_auc, 'b')\n",
    "    plt.legend(['Train AUC','Validation AUC'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('AUC')\n",
    "\n",
    "    epoch = np.arange(1,len(train_acc)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "    plt.legend(['Train Accuracy','Validation Accuracy'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('Acc')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    #-------- Save results --------#\n",
    "\n",
    "    results = pd.DataFrame(list(zip( (int(x) for x in val_targs), (int(x) for x in val_preds))),columns =['target', 'pred'])\n",
    "    print(results)\n",
    "\n",
    "    #results.to_csv('results/df_targets_preds_th.csv'.format(str(date.today())), index=False)\n",
    "    \n",
    "    \n",
    "    #-------- Performance Evaluation --------#\n",
    "    # The results change every time we train, we should check why (maybe we missed something or did wrong with the seeds?)\n",
    "\n",
    "    print(\"AUC: \", roc_auc_score(results['target'], results['pred']))\n",
    "    print(\"MCC: \", matthews_corrcoef(results['target'], results['pred']))\n",
    "\n",
    "    confusion_matrix = pd.crosstab(results['target'], results['pred'], rownames=['Actual'], colnames=['Predicted'])\n",
    "    sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot roc curve\n",
    "\n",
    "    fpr, tpr, thres = roc_curve(results['target'], results['pred'])\n",
    "    print('AUC: {:.3f}'.format(roc_auc_score(results['target'], results['pred'])))\n",
    "\n",
    "    print( len([i for i, (a, b) in enumerate(zip(results['pred'], results['target'])) if a != b]))\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    # roc curve\n",
    "    plt.plot(fpr, tpr, \"b\", label='ROC Curve')\n",
    "    plt.plot([0,1],[0,1], \"k--\", label='Random Guess')\n",
    "    plt.xlabel(\"false positive rate\")\n",
    "    plt.ylabel(\"true positive rate\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"ROC curve\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    AUC = roc_auc_score(results['target'], results['pred'])\n",
    "    MCC = matthews_corrcoef(results['target'], results['pred'])\n",
    "    print(\"AUC: \", AUC)\n",
    "    print(\"MCC: \", MCC)\n",
    "    print(\"early stop:\", early_stop)\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing values\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('embedding', embedding) \n",
    "    mlflow.log_param('Hidden Neurons', numHN)\n",
    "    mlflow.log_param('filters CNN', numFilter)\n",
    "    mlflow.log_param('Dropout rate', dropOutRate)\n",
    "    mlflow.log_metric('AUC', AUC)\n",
    "    mlflow.log_metric('MCC', MCC)\n",
    "    #ADD ARTIFACTS (PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebcc79-4feb-4f4f-bf49-0271c1dcacac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e764bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2802b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
