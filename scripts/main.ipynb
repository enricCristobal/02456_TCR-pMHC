{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c8da296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae5f4635",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'esm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-0e76910d59e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\enric\\Documents\\Bioinformatics\\3rd semester\\Deep learning\\Project\\02456_TCR-pMHC\\scripts\\encoding.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mesm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mesm_1b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpeptides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'esm'"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import encoding as enc\n",
    "import model\n",
    "import functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43124096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs available. Using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) avalable.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPUs available. Using CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "254e4329",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Seeds --------#\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37e1c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Parameters --------#\n",
    "\n",
    "# Just an example below, we can always change later\n",
    "EPOCHS = 1000\n",
    "MINI_BATCH_SIZE = 512\n",
    "LEARNING_RATE = 0.0001\n",
    "PATIENCE = EPOCHS // 10\n",
    "N_HIDDEN_NEURONS = 128\n",
    "\n",
    "# Can be converted to BCE or another\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "DATADIR = \"/data/\"\n",
    "MATRICES = \"/data/Matrices\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5d283820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#-------- Unzip Train --------#\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(TRAINDIR)) != 0:\n",
    "        print(\"{} already exist.\".format(TRAINDIR))\n",
    "except:\n",
    "    !unzip ../data/train.zip -d ../data/train\n",
    "\n",
    "    \n",
    "#-------- Unzip Validation --------#\n",
    "try:\n",
    "    if len(os.listdir(VALIDATIONDIR)) != 0:\n",
    "        print(\"{} already exist.\".format(VALIDATIONDIR))\n",
    "except:\n",
    "    !unzip ../data/validation.zip -d ../data/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edd67086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: ['P2_labels.npz', 'P3_input.npz', 'P4_input.npz', 'P2_input.npz', '__MACOSX', 'P1_input.npz', 'P3_labels.npz', 'P4_labels.npz', 'P1_labels.npz'] \n",
      "\n",
      "validation: ['P4_input.npz', '__MACOSX', 'P4_labels.npz']\n"
     ]
    }
   ],
   "source": [
    "print(\"train:\", os.listdir('../data/train'), '\\n')\n",
    "print(\"validation:\",os.listdir('../data/validation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4a3e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: 4174 420 54\n",
      "val set shape: 1532 420 54\n",
      "Percent positive samples in train: 24.96406324868232\n",
      "Percent positive samples in val: 25.0\n",
      "\n",
      "NOTE:\n",
      "Setting batch-size to 64\n",
      "Using device (CPU/GPU): cpu\n"
     ]
    }
   ],
   "source": [
    "#-------- Import Dataset --------#\n",
    "\n",
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "import glob\n",
    "for fp in glob.glob(\"../data/train/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "#print(len(data_list))\n",
    "#print(len(target_list))\n",
    "\n",
    "X_train = np.concatenate(data_list[ :-1])\n",
    "y_train = np.concatenate(target_list[:-1])\n",
    "nsamples, nx, ny = X_train.shape\n",
    "print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "X_val = np.concatenate(data_list[-1: ])\n",
    "y_val = np.concatenate(target_list[-1: ])\n",
    "nsamples, nx, ny = X_val.shape\n",
    "print(\"val set shape:\", nsamples,nx,ny)\n",
    "\n",
    "p_neg = len(y_train[y_train == 1])/len(y_train)*100\n",
    "print(\"Percent positive samples in train:\", p_neg)\n",
    "\n",
    "p_pos = len(y_val[y_val == 1])/len(y_val)*100\n",
    "print(\"Percent positive samples in val:\", p_pos)\n",
    "\n",
    "# make the data set into one dataset that can go into dataloader\n",
    "train_ds = []\n",
    "for i in range(len(X_train)):\n",
    "    train_ds.append([np.transpose(X_train[i]), y_train[i]])\n",
    "\n",
    "val_ds = []\n",
    "for i in range(len(X_val)):\n",
    "    val_ds.append([np.transpose(X_val[i]), y_val[i]])\n",
    "\n",
    "bat_size = 64\n",
    "print(\"\\nNOTE:\\nSetting batch-size to\", bat_size)\n",
    "train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device (CPU/GPU):\", device)\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing network\n"
     ]
    }
   ],
   "source": [
    "#-------- Define network --------#\n",
    "\n",
    "print(\"Initializing network\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 420\n",
    "num_classes = 1\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialize network\n",
    "from model import Net\n",
    "net = Net(num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "\n",
      "Epoch: 1\n",
      "Training loss: 0.00769993057474494 Validation loss: 0.007803560234606266\n",
      "MCC Train: 0.430046049926052 MCC val: 0.4381855638265855\n",
      "\n",
      "Epoch: 2\n",
      "Training loss: 0.007039184682071209 Validation loss: 0.00878058560192585\n",
      "MCC Train: 0.5069319945480502 MCC val: 0.4078798191616063\n",
      "\n",
      "Epoch: 3\n",
      "Training loss: 0.006805058103054762 Validation loss: 0.007479188498109579\n",
      "MCC Train: 0.5243772935095986 MCC val: 0.4913678541941079\n",
      "\n",
      "Epoch: 4\n",
      "Training loss: 0.006943217478692532 Validation loss: 0.007849294692277908\n",
      "MCC Train: 0.4964261068738395 MCC val: 0.4728387880375803\n",
      "\n",
      "Epoch: 5\n",
      "Training loss: 0.006267018150538206 Validation loss: 0.009424594230949879\n",
      "MCC Train: 0.567543437154431 MCC val: 0.33107035174996935\n",
      "\n",
      "Finished Training ...\n"
     ]
    }
   ],
   "source": [
    "#-------- Train network --------#\n",
    "\n",
    "print(\"Training\")\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    cur_loss = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    net.train()\n",
    "    train_preds, train_targs = [], [] \n",
    "    for batch_idx, (data, target) in enumerate(train_ldr):\n",
    "        X_batch =  data.float().detach().requires_grad_(True)\n",
    "        target_batch = torch.tensor(np.array(target), dtype = torch.float).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = net(X_batch)\n",
    "        \n",
    "        batch_loss = criterion(output, target_batch)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        preds = np.round(output.detach().cpu())\n",
    "        train_targs += list(np.array(target_batch.cpu()))\n",
    "        train_preds += list(preds.data.numpy().flatten())\n",
    "        cur_loss += batch_loss.detach()\n",
    "\n",
    "    losses.append(cur_loss / len(train_ldr.dataset))\n",
    "        \n",
    "    \n",
    "    net.eval()\n",
    "    ### Evaluate validation\n",
    "    val_preds, val_targs = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_ldr): ###\n",
    "            x_batch_val = data.float().detach()\n",
    "            y_batch_val = target.float().detach().unsqueeze(1)\n",
    "            \n",
    "            output = net(x_batch_val)\n",
    "            \n",
    "            val_batch_loss = criterion(output, y_batch_val)\n",
    "            \n",
    "            preds = np.round(output.detach())\n",
    "            val_preds += list(preds.data.numpy().flatten()) \n",
    "            val_targs += list(np.array(y_batch_val))\n",
    "            val_loss += val_batch_loss.detach()\n",
    "            \n",
    "        val_losses.append(val_loss / len(val_ldr.dataset))\n",
    "        print(\"\\nEpoch:\", epoch+1)\n",
    "        \n",
    "        train_acc_cur = accuracy_score(train_targs, train_preds)  \n",
    "        valid_acc_cur = accuracy_score(val_targs, val_preds) \n",
    "\n",
    "        train_acc.append(train_acc_cur)\n",
    "        valid_acc.append(valid_acc_cur)\n",
    "        \n",
    "        from sklearn.metrics import matthews_corrcoef\n",
    "        print(\"Training loss:\", losses[-1].item(), \"Validation loss:\", val_losses[-1].item(), end = \"\\n\")\n",
    "        print(\"MCC Train:\", matthews_corrcoef(train_targs, train_preds), \"MCC val:\", matthews_corrcoef(val_targs, val_preds))\n",
    "        \n",
    "print('\\nFinished Training ...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
