{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a01d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Libraries --------#\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3423c37a-4b59-4a1f-93f8-4ef70f490869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Modules from project--------#\n",
    "import encoding as enc\n",
    "from model import Net, Net_th, Net_project\n",
    "import functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4211d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Set Device --------#\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPUs available. Using CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5adc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Seeds --------#\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f33ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Directories --------#\n",
    "\n",
    "DATADIR = '/data/'\n",
    "TRAINDIR = '../data/train'\n",
    "VALIDATIONDIR = '../data/validation'\n",
    "MATRICES = '/data/Matrices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f45575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Unzip Train --------#\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(TRAINDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(TRAINDIR))\n",
    "except:\n",
    "    !unzip ../data/train.zip -d ../data/train\n",
    "\n",
    "    \n",
    "#-------- Unzip Validation --------#\n",
    "\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(VALIDATIONDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(VALIDATIONDIR))\n",
    "except:\n",
    "    !unzip ../data/validation.zip -d ../data/validation\n",
    "    \n",
    "print('Train directory:\\n\\n', '\\n'.join(str(p) for p in os.listdir(TRAINDIR)), '\\n\\n')\n",
    "print('Validation directory:\\n\\n','\\n'.join(str(p) for p in os.listdir(VALIDATIONDIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b37f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Dataset --------#             #TO UPDATE: NO REPEAT P4 AND CONSERVE PARTITION FOR CROSS VAL\n",
    "\n",
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "import glob\n",
    "for fp in glob.glob(\"../data/train/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "print(len(data_list))\n",
    "print(len(target_list))\n",
    "\n",
    "X_train = np.concatenate(data_list[ :-1])\n",
    "y_train = np.concatenate(target_list[:-1])\n",
    "nsamples, nx, ny = X_train.shape\n",
    "print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "X_val = np.concatenate(data_list[-1: ])\n",
    "y_val = np.concatenate(target_list[-1: ])\n",
    "nsamples, nx, ny = X_val.shape\n",
    "print(\"Val set shape:\", nsamples,nx,ny)\n",
    "\n",
    "p_neg = len(y_train[y_train == 1])/len(y_train)*100\n",
    "print(\"Percent positive samples in train:\", p_neg)\n",
    "\n",
    "p_pos = len(y_val[y_val == 1])/len(y_val)*100\n",
    "print(\"Percent positive samples in val:\", p_pos)\n",
    "\n",
    "# make the data set into one dataset that can go into dataloader\n",
    "train_ds = []\n",
    "for i in range(len(X_train)):\n",
    "    train_ds.append([np.transpose(X_train[i]), y_train[i]])\n",
    "\n",
    "val_ds = []\n",
    "for i in range(len(X_val)):\n",
    "    val_ds.append([np.transpose(X_val[i]), y_val[i]])\n",
    "\n",
    "bat_size = 64\n",
    "print(\"\\nNOTE:\\nSetting batch-size to\", bat_size)\n",
    "train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9897fc5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#-------- Select the netwrok you would like to use, thesis or hecaton code -------#\n",
    "\n",
    "CNN = False # ONLY CNN\n",
    "CNN_RNN = True # CNN + RNN\n",
    "\n",
    "# Hyperparameters to fine-tune\n",
    "embedding = \"Baseline\"\n",
    "numHN=64\n",
    "numFilter=100\n",
    "dropOutRate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46dc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding of data\n",
    "\n",
    "#create directory to fetch/store embedded\n",
    "embedding_dir= '../data/embeddedFiles/'\n",
    "try:\n",
    "    os.mkdir(embedding_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#try to fecth if already exist\n",
    "try:\n",
    "    with open(embedding_dir+'trainX-{}'.format(embedding)) as f:\n",
    "        X_train =  pickle.load(f)\n",
    "    with open(embedding_dir+'testX-{}'.format(embedding)) as f:\n",
    "        X_test =  pickle.load(f)\n",
    "\n",
    "#if no prior file, then embbed:\n",
    "except:\n",
    "    if embedding == \"Baseline\":\n",
    "        pass #this corresponds to original formatting\n",
    "\n",
    "    elif embedding == \"esm-1b\":\n",
    "        X_train = func.extract_sequences(X_train, merge=True).values.tolist()\n",
    "        X_train = [enc.esm_1b(seq, pooling=False) for seq in X_train]\n",
    "\n",
    "        X_test = func.extract_sequences(X_test, merge=True).values.tolist()\n",
    "        X_test = [enc.esm_1b(seq, pooling=False) for seq in X_test]\n",
    "\n",
    "        #save\n",
    "        with open(embedding_dir+'trainX-{}'.format(embedding)) as f:\n",
    "            pickle.dump(X_train, f)\n",
    "        with open(embedding_dir+'testX-{}'.format(embedding)) as f:\n",
    "            pickle.dump(X_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed hyperparameters\n",
    "features = list(range(54))  #to be redefined\n",
    "residues = list(range(416))  #to be redefined\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "n_features = len(features)\n",
    "input_size = len(residues)\n",
    "bat_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data load\n",
    "\n",
    "X_train = np.concatenate(data_list[ :-1])\n",
    "y_train = np.concatenate(target_list[:-1])\n",
    "nsamples, nx, ny = X_train.shape\n",
    "print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "X_valid = np.concatenate(data_list[-1: ])\n",
    "y_valid = np.concatenate(target_list[-1: ])\n",
    "nsamples, nx, ny = X_valid.shape\n",
    "print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "# Dataloader\n",
    "train_ds = []\n",
    "for i in range(len(X_train)):\n",
    "    train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "val_ds = []\n",
    "for i in range(len(X_valid)):\n",
    "    val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c5cfd-f839-4575-a8ae-eb58a5b4bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "###    CNN Code (hecaton)   ###\n",
    "###############################\n",
    "\n",
    "if CNN:\n",
    "    \n",
    "    #-------- Define network --------#\n",
    "\n",
    "    print(\"Initializing network\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_size = 420\n",
    "    num_classes = 1\n",
    "    learning_rate = 0.01\n",
    "    epochs = 10\n",
    "\n",
    "    # Initialize network\n",
    "    net = Net(num_classes=num_classes).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    #-------- Train network --------#\n",
    "\n",
    "    print(\"Training\")\n",
    "\n",
    "    train_acc, train_loss = [], []\n",
    "    valid_acc, valid_loss = [], []\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        cur_loss = 0\n",
    "        val_loss = 0\n",
    "\n",
    "        net.train()\n",
    "        train_preds, train_targs = [], [] \n",
    "        for batch_idx, (data, target) in enumerate(train_ldr):\n",
    "            X_batch =  data.float().detach().requires_grad_(True)\n",
    "            target_batch = torch.tensor(np.array(target), dtype = torch.float).unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = net(X_batch)\n",
    "\n",
    "            batch_loss = criterion(output, target_batch)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = np.round(output.detach().cpu())\n",
    "            train_targs += list(np.array(target_batch.cpu()))\n",
    "            train_preds += list(preds.data.numpy().flatten())\n",
    "            cur_loss += batch_loss.detach()\n",
    "\n",
    "        losses.append(cur_loss / len(train_ldr.dataset))\n",
    "        net.eval()\n",
    "\n",
    "        ### Evaluate validation\n",
    "        val_preds, val_targs = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_ldr): ###\n",
    "                x_batch_val = data.float().detach()\n",
    "                y_batch_val = target.float().detach().unsqueeze(1)\n",
    "\n",
    "                output = net(x_batch_val)\n",
    "\n",
    "                val_batch_loss = criterion(output, y_batch_val)\n",
    "\n",
    "                preds = np.round(output.detach())\n",
    "                val_preds += list(preds.data.numpy().flatten()) \n",
    "                val_targs += list(np.array(y_batch_val))\n",
    "                val_loss += val_batch_loss.detach()\n",
    "\n",
    "            val_losses.append(val_loss / len(val_ldr.dataset))\n",
    "            print(\"\\nEpoch:\", epoch+1)\n",
    "\n",
    "            train_acc_cur = accuracy_score(train_targs, train_preds)  \n",
    "            valid_acc_cur = accuracy_score(val_targs, val_preds) \n",
    "\n",
    "            train_acc.append(train_acc_cur)\n",
    "            valid_acc.append(valid_acc_cur)\n",
    "\n",
    "    \n",
    "            print(\"Training loss:\", round(losses[-1].item(),3), \"Validation loss:\", round(val_losses[-1].item(),3), end = \"\\n\")\n",
    "            print(\"MCC Train:\", round(matthews_corrcoef(train_targs, train_preds),3), \"MCC val:\", round(matthews_corrcoef(val_targs, val_preds),3) )\n",
    "\n",
    "    print('\\nFinished Training ...')\n",
    "\n",
    "\n",
    "    #-------- Save results --------#\n",
    "\n",
    "    results = pd.DataFrame(list(zip( (x[0] for x in val_targs), val_preds)),columns =['target', 'pred'])\n",
    "    print(results)\n",
    "\n",
    "    #results.to_csv('results/df_targets_preds.csv'.format(str(date.today())), index=False)\n",
    "\n",
    "\n",
    "    #-------- Performance Evaluation --------#\n",
    "    # The results change every time we train, we should check why (maybe we missed something or did wrong with the seeds?)\n",
    "\n",
    "    print(\"AUC: \", roc_auc_score(results['target'], results['pred']))\n",
    "    print(\"MCC: \", matthews_corrcoef(results['target'], results['pred']))\n",
    "\n",
    "    confusion_matrix = pd.crosstab(results['target'], results['pred'], rownames=['Actual'], colnames=['Predicted'])\n",
    "    sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################\n",
    "###    CNN+RNN (thesis)     ###\n",
    "###############################\n",
    "\n",
    "if CNN_RNN:\n",
    "    \n",
    "    #-------- Initials --------#\n",
    "\n",
    "    # All features\n",
    "    features = list(range(54))\n",
    "    residues = list(range(416))\n",
    "    input_size = len(residues)\n",
    "    num_classes = 1\n",
    "    learning_rate = 0.001\n",
    "    bat_size = 128\n",
    "    epochs = 100\n",
    "    n_features = len(features)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "\n",
    "    #-------- Train --------#\n",
    "    X_train = np.concatenate(data_list[ :-1])\n",
    "    y_train = np.concatenate(target_list[:-1])\n",
    "    nsamples, nx, ny = X_train.shape\n",
    "    print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "    X_valid = np.concatenate(data_list[-1: ])\n",
    "    y_valid = np.concatenate(target_list[-1: ])\n",
    "    nsamples, nx, ny = X_valid.shape\n",
    "    print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "    # Dataloader\n",
    "    train_ds = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "    val_ds = []\n",
    "    for i in range(len(X_valid)):\n",
    "        val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "    train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "    val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)\n",
    "\n",
    "    # Initialize network\n",
    "    net = Net_th(num_classes=num_classes, n_features=n_features).to(device)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate,\n",
    "                           weight_decay=0.0005,\n",
    "                           amsgrad=True,)\n",
    "    train_acc, train_losses, train_auc, valid_acc, valid_losses, valid_auc, val_preds, val_targs = func.train_project(net, optimizer, train_ldr, val_ldr, [], X_valid, epochs, criterion)\n",
    "\n",
    "\n",
    "    #-------- Performance --------#\n",
    "    epoch = np.arange(1,len(train_losses)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_losses, 'r', epoch, valid_losses, 'b')\n",
    "    plt.legend(['Train Loss','Validation Loss'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "\n",
    "    epoch = np.arange(1,len(train_auc)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_auc, 'r', epoch, valid_auc, 'b')\n",
    "    plt.legend(['Train AUC','Validation AUC'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('AUC')\n",
    "\n",
    "    epoch = np.arange(1,len(train_acc)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "    plt.legend(['Train Accuracy','Validation Accuracy'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('Acc')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    #-------- Save results --------#\n",
    "\n",
    "    results = pd.DataFrame(list(zip( (int(x) for x in val_targs), (int(x) for x in val_preds))),columns =['target', 'pred'])\n",
    "    print(results)\n",
    "\n",
    "    #results.to_csv('results/df_targets_preds_th.csv'.format(str(date.today())), index=False\n",
    "\n",
    "    \n",
    "    #-------- Performance Evaluation --------#\n",
    "    # The results change every time we train, we should check why (maybe we missed something or did wrong with the seeds?)\n",
    "\n",
    "    print(\"AUC: \", roc_auc_score(results['target'], results['pred']))\n",
    "    print(\"MCC: \", matthews_corrcoef(results['target'], results['pred']))\n",
    "\n",
    "    confusion_matrix = pd.crosstab(results['target'], results['pred'], rownames=['Actual'], colnames=['Predicted'])\n",
    "    sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.show()\n",
    "\n",
    "    print( len([i for i, (a, b) in enumerate(zip(results['pred'], results['target'])) if a != b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing values\n",
    "with mlflow.start:run():\n",
    "    mlflow.log_param('embedding', embedding) \n",
    "    mlflow.log_param('Hidden Neurons', numHN)\n",
    "    mlflow.log_param('filters CNN', numFilter)\n",
    "    mlflow.log_param('Dropout rate', dropOutRate)\n",
    "    mlflow.log_metric('AUC', AUC)\n",
    "    mlflow.log_metric('MCC', MCC)\n",
    "    #ADD ARTIFACTS (PLOTS)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
