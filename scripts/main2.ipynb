{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "151fdcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Libraries --------#\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3423c37a-4b59-4a1f-93f8-4ef70f490869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Modules from project--------#\n",
    "import encoding as enc\n",
    "from model import Net, Net_thesis, Net_project\n",
    "import functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4211d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs available. Using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "#-------- Set Device --------#\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPUs available. Using CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5adc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Seeds --------#\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f33ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Modules from project--------#\n",
    "import encoding as enc\n",
    "from model import Net_project\n",
    "import functions as func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b37f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#-------- Import Dataset --------#             \n",
    "\n",
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "import glob\n",
    "for fp in glob.glob(\"../data/train/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "for fp in glob.glob(\"../data/validation/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "print(len(data_list))\n",
    "print(len(target_list))\n",
    "\n",
    "data_partitions = len(data_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9897fc5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#-------- Select the network you would like to use -------#\n",
    "\n",
    "CNN = False # ONLY CNN\n",
    "CNN_RNN = True # CNN + RNN\n",
    "\n",
    "# Hyperparameters to fine-tune\n",
    "embedding = \"Baseline\"\n",
    "numHN=32\n",
    "numFilter=100\n",
    "dropOutRate=0.1\n",
    "\n",
    "##--- parameters fixed\n",
    "keep_energy=True\n",
    "cross_validation = False\n",
    "bat_size = 128\n",
    "num_classes=1\n",
    "learning_rate=0.001\n",
    "epochs = 100\n",
    "patience=10\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f46dc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding of data\n",
    "\n",
    "#create directory to fetch/store embedded\n",
    "embedding_dir= '../data/embeddedFiles/'\n",
    "try:\n",
    "    os.mkdir(embedding_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "#try to fecth if already exist\n",
    "if embedding == \"Baseline\":   \n",
    "    data_list_enc = data_list\n",
    "else:\n",
    "    try:\n",
    "        infile = open(embedding_dir+'dataset-{}'.format(embedding), 'rb')\n",
    "        data_list_enc =  pickle.load(infile)\n",
    "        infile.close()\n",
    "\n",
    "    #if no prior file, use encoder script to encode:\n",
    "    except:\n",
    "        print(\"embedded file not found\")\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6210d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1526\n",
      "420\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "print(len(data_list_enc))\n",
    "print(len(data_list_enc[0]))\n",
    "print(len(data_list_enc[0][0]))\n",
    "print(len(data_list_enc[0][0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2bcc313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add energy terms from original dataset         \n",
    "if keep_energy:\n",
    "    if embedding == \"Baseline\":\n",
    "        energy_set =''\n",
    "        pad = 0\n",
    "        pass\n",
    "    else:\n",
    "        for i in range (len(data_list_enc)):\n",
    "            energy_set = func.extract_energy_terms(data_list[i]) \n",
    "            for j in range(0, len(energy_set)):\n",
    "                pad = 420 - len(energy_set[j])\n",
    "                energy_set[j] = np.pad(energy_set[j], ((0, pad), (0, 0)), 'constant')\n",
    "                data_list_enc[i][j] = np.concatenate((data_list_enc[i][j], energy_set[j]), axis=1)\n",
    "\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "949ce6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1526\n",
      "420\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "print(len(data_list_enc))\n",
    "print(len(data_list_enc[0]))\n",
    "print(len(data_list_enc[0][0]))\n",
    "print(len(data_list_enc[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2cdcea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del data_list, energy_set, pad\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52ef0161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: 4174 420 54\n",
      "Validation set shape: 1532 420 54\n",
      "Test set shape: 1207 420 54\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate(data_list_enc[0:3])\n",
    "y_train = np.concatenate(target_list[0:3])\n",
    "nsamples, nx, ny = X_train.shape\n",
    "print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "X_valid = np.concatenate(data_list_enc[3:4])\n",
    "y_valid = np.concatenate(target_list[3:4])\n",
    "nsamples, nx, ny = X_valid.shape\n",
    "print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "\n",
    "X_test = np.concatenate(data_list_enc[4:])\n",
    "y_test = np.concatenate(target_list[4:])\n",
    "nsamples, nx, ny = X_test.shape\n",
    "print(\"Test set shape:\", nsamples,nx,ny)\n",
    "\n",
    "# features and residues\n",
    "features = list(range(ny))\n",
    "residues = list(range(nx)) \n",
    "n_features = len(features)\n",
    "input_size = len(residues)\n",
    "\n",
    "# Dataloader\n",
    "train_ds = []\n",
    "for i in range(len(X_train)):\n",
    "    train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "val_ds = []\n",
    "for i in range(len(X_valid)):\n",
    "    val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "test_ds = []\n",
    "for i in range(len(X_test)):\n",
    "    test_ds.append([np.transpose(X_test[i][:,features]), y_test[i]])\n",
    "    \n",
    "    \n",
    "train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)\n",
    "test_ldr = torch.utils.data.DataLoader(test_ds,batch_size=bat_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1153ff8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "cross_validation False\n",
      "embedding Baseline\n",
      "numHN 32\n",
      "numFilter 100\n",
      "dropOutRate 0.1\n",
      "keep_energy True\n",
      "num_classes 1\n",
      "learning_rate 0.001\n",
      "bat_size 128\n",
      "patience 10\n",
      "criterion BCEWithLogitsLoss()\n",
      "\n",
      "\n",
      "\n",
      "Train\n",
      "Epoch 0  \t Train loss: 0.00018 \t Validation loss: 0.00582\n",
      "Epoch 10  \t Train loss: 0.00013 \t Validation loss: 0.00451\n",
      "Epoch 20  \t Train loss: 0.00013 \t Validation loss: 0.00415\n",
      "Epoch 30  \t Train loss: 0.00012 \t Validation loss: 0.00404\n",
      "Epoch 40  \t Train loss: 0.00012 \t Validation loss: 0.00388\n",
      "Epoch 50  \t Train loss: 0.00011 \t Validation loss: 0.00362\n",
      "Epoch 60  \t Train loss: 0.00010 \t Validation loss: 0.00350\n",
      "Epoch 70  \t Train loss: 0.00009 \t Validation loss: 0.00341\n",
      "Epoch 80  \t Train loss: 0.00007 \t Validation loss: 0.00346\n",
      "Early stopping\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "Done in 11.16 mins.\n",
      "test_loss, test_acc, test_auc:\n",
      "0.38532382249832153 , 0.875 , 0.8142857142857143\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################\n",
    "###    CNN+RNN (thesis)     ###\n",
    "###############################\n",
    "start = time.time()\n",
    "\n",
    "if cross_validation == False:\n",
    "    \n",
    "    print(\"Parameters:\")\n",
    "    print(\"cross_validation\", cross_validation)\n",
    "    print(\"embedding\", embedding)\n",
    "    print(\"numHN\", numHN)\n",
    "    print(\"numFilter\", numFilter)\n",
    "    print(\"dropOutRate\", dropOutRate)\n",
    "    print(\"keep_energy\", keep_energy)\n",
    "    print(\"num_classes\", num_classes)\n",
    "    print(\"learning_rate\", learning_rate)\n",
    "    print(\"bat_size\", bat_size)\n",
    "    print(\"patience\", patience)\n",
    "    print(\"criterion\", criterion)\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    #-------- Train --------#\n",
    "\n",
    "    print(\"Train\")\n",
    "    # Dataloader\n",
    "    train_ds = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "    val_ds = []\n",
    "    for i in range(len(X_valid)):\n",
    "        val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "    train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=len(train_ds), shuffle=True)\n",
    "    val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)\n",
    "\n",
    "    # Initialize network\n",
    "    net = Net_project(num_classes=num_classes, \n",
    "             n_features=n_features, \n",
    "             numHN=numHN, \n",
    "             numFilter=numFilter,\n",
    "             dropOutRate=dropOutRate).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate,\n",
    "                           weight_decay=0.0005,\n",
    "                           amsgrad=True,)\n",
    "    \n",
    "    train_acc, train_losses, train_auc, valid_acc, valid_losses, valid_auc, val_preds, val_targs, test_preds, test_targs, test_loss, test_acc, test_auc = func.train_project(net, optimizer, train_ldr, val_ldr, test_ldr, X_valid, epochs, criterion, patience)\n",
    "\n",
    "else:\n",
    "    pass\n",
    "\n",
    "print(\"Done in\", round((time.time()-start)/60,2), \"mins.\" )\n",
    "\n",
    "print(\"test_loss, test_acc, test_auc:\")\n",
    "print(test_loss.item(), \",\", test_acc[0], \",\", test_auc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f75bb91f-7dde-48da-a051-d6a5acf9b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8142857142857143\n",
      "MCC:  0.875\n"
     ]
    }
   ],
   "source": [
    "#metrics\n",
    "#AUC = roc_auc_score(results['target'], results['pred'])\n",
    "#MCC = matthews_corrcoef(results['target'], results['pred'])\n",
    "print(\"AUC: \", test_auc[0])\n",
    "print(\"MCC: \", test_acc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8218f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing values\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('embedding', embedding) \n",
    "    mlflow.log_param('Hidden Neurons', numHN)\n",
    "    mlflow.log_param('filters CNN', numFilter)\n",
    "    mlflow.log_param('Dropout rate', dropOutRate)\n",
    "    mlflow.log_metric('AUC', test_auc[0])\n",
    "    mlflow.log_metric('MCC', test_acc[0])\n",
    "    #ADD ARTIFACTS (PLOTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ac642",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
