{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a01d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Libraries --------#\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dafad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Modules from project--------#\n",
    "import encoding as enc\n",
    "from model import Net, Net_th\n",
    "import functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4211d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs available. Using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "#-------- Set Device --------#\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPUs available. Using CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd5adc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Seeds --------#\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f33ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Directories --------#\n",
    "\n",
    "DATADIR = '/data/'\n",
    "TRAINDIR = '../data/train'\n",
    "VALIDATIONDIR = '../data/validation'\n",
    "MATRICES = '/data/Matrices'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f45575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train already unzipped.\n",
      "../data/validation already unzipped.\n",
      "Train directory:\n",
      "\n",
      " P1_input.npz\n",
      "P1_labels.npz\n",
      "P2_input.npz\n",
      "P2_labels.npz\n",
      "P3_input.npz\n",
      "P3_labels.npz\n",
      "P4_input.npz\n",
      "P4_labels.npz\n",
      "__MACOSX \n",
      "\n",
      "\n",
      "Validation directory:\n",
      "\n",
      " P4_input.npz\n",
      "P4_labels.npz\n",
      "__MACOSX\n"
     ]
    }
   ],
   "source": [
    "#-------- Unzip Train --------#\n",
    "\n",
    "if len(os.listdir(TRAINDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(TRAINDIR))\n",
    "else:\n",
    "    !unzip ../data/train.zip -d ../data/train\n",
    "\n",
    "    \n",
    "#-------- Unzip Validation --------#\n",
    "\n",
    "\n",
    "if len(os.listdir(VALIDATIONDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(VALIDATIONDIR))\n",
    "else:\n",
    "    !unzip ../data/validation.zip -d ../data/validation\n",
    "    \n",
    "print('Train directory:\\n\\n', '\\n'.join(str(p) for p in os.listdir(TRAINDIR)), '\\n\\n')\n",
    "print('Validation directory:\\n\\n','\\n'.join(str(p) for p in os.listdir(VALIDATIONDIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b37f634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n",
      "Training set shape: 4174 420 54\n",
      "Val set shape: 1532 420 54\n",
      "Percent positive samples in train: 24.96406324868232\n",
      "Percent positive samples in val: 25.0\n",
      "\n",
      "NOTE:\n",
      "Setting batch-size to 64\n"
     ]
    }
   ],
   "source": [
    "#-------- Import Dataset --------#             #TO UPDATE: NO REPEAT P4 AND CONSERVE PARTITION FOR CROSS VAL\n",
    "\n",
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "import glob\n",
    "for fp in glob.glob(\"../data/train/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "print(len(data_list))\n",
    "print(len(target_list))\n",
    "\n",
    "X_train = np.concatenate(data_list[ :-1])\n",
    "y_train = np.concatenate(target_list[:-1])\n",
    "nsamples, nx, ny = X_train.shape\n",
    "print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "X_val = np.concatenate(data_list[-1: ])\n",
    "y_val = np.concatenate(target_list[-1: ])\n",
    "nsamples, nx, ny = X_val.shape\n",
    "print(\"Val set shape:\", nsamples,nx,ny)\n",
    "\n",
    "p_neg = len(y_train[y_train == 1])/len(y_train)*100\n",
    "print(\"Percent positive samples in train:\", p_neg)\n",
    "\n",
    "p_pos = len(y_val[y_val == 1])/len(y_val)*100\n",
    "print(\"Percent positive samples in val:\", p_pos)\n",
    "\n",
    "# make the data set into one dataset that can go into dataloader\n",
    "train_ds = []\n",
    "for i in range(len(X_train)):\n",
    "    train_ds.append([np.transpose(X_train[i]), y_train[i]])\n",
    "\n",
    "val_ds = []\n",
    "for i in range(len(X_val)):\n",
    "    val_ds.append([np.transpose(X_val[i]), y_val[i]])\n",
    "\n",
    "bat_size = 64\n",
    "print(\"\\nNOTE:\\nSetting batch-size to\", bat_size)\n",
    "train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9897fc5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    " # Hyperparameters to fine-tune\n",
    "embedding = \"Baseline\"\n",
    "numHN=64\n",
    "numFilter=100\n",
    "dropOutRate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f46dc5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4174, 420, 54)\n",
      "4174\n",
      "data:  [('', 'GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLGILGFVFTLEQSPQFLSIQEGENLTVYCNSSSVFSSLQWYRQEPGEGPVLLVTVVTGGEVKKLKRLTFQFGDARKDSSLHITAAQPGDTGLYLCAGAWNTGKLIFGQGTTLQVKPGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLIYYSQIVNDFQKGDIAEGYSVSREKKESFPLTVTSAQKNPTAFYLCASSIRSSYEQYFGPGT')]\n",
      "data:  [('', 'GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLGILGFVFTLEQSPQFLSIQEGENLTVYCNSSSVFSSLQWYRQEPGEGPVLLVTVVTGGEVKKLKRLTFQFGDARKDSSLHITAAQPGDTGLYLCAGAGSQGNLIFGKGTKLSVKPGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQVPGQGLRLIYYSHIVNDFQKGDIAEGYSVSREKKESFPLTVTSAQKNPTAFYLCASSSRSAYEQYFGPGT')]\n",
      "data:  [('', 'GSHSMRYFFTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDGETRKVKAHSQTHRVDLGTLRGYYNQSEAGSHTVQRMYGCDVGSDWRFLRGYHQYAYDGKDYIALKEDLRSWTAADMAAQTTKHKWEAAHVAEQLRAYLEGTCVEWLRRYLENGKETLGILGFVFTLQQVKQNSPSLSVQEGRISILNCDYTNSMFDYFLWYKKYPAEGPTFLISISSIKDKNEDGRFTVFLNKSAKHLSLHIVPSQPGDSAVYFCAASGSQGNLIFGKGTKLSVKPGITQSPKYLFRKEGQNVTLSCEQNLNHDAMYWYRQDPGQGLRLIYYSQIVNDFQKGDIAEGYSVSREKKESFPLTVTSAQKNPTAFYLCASSIRSSYEQYFGPGT')]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-5ab0081efa13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mesm_1b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#[reverseOneHot(arr[:,0:20]) for arr in dataset_X]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-5ab0081efa13>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mesm_1b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#[reverseOneHot(arr[:,0:20]) for arr in dataset_X]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\dev\\02456_TCR-pMHC\\scripts\\encoding.py\u001b[0m in \u001b[0;36mesm_1b\u001b[1;34m(peptides, pooling)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Load pre-trained ESM-1b model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malphabet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mesm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mesm1b_t33_650M_UR50S\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mbatch_converter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malphabet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_batch_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\esm\\pretrained.py\u001b[0m in \u001b[0;36mesm1b_t33_650M_UR50S\u001b[1;34m()\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAlphabet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m     \"\"\"\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_and_alphabet_hub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"esm1b_t33_650M_UR50S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mesm_msa1_t12_100M_UR50S\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\esm\\pretrained.py\u001b[0m in \u001b[0;36mload_model_and_alphabet_hub\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_model_and_alphabet_hub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"https://dl.fbaipublicfiles.com/fair-esm/models/{model_name}.pt\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[0mmodel_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_hub_workaround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_has_regression_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mregression_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_regression_hub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\esm\\pretrained.py\u001b[0m in \u001b[0;36mload_hub_workaround\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_hub_workaround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict_from_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# Pytorch version issue - see https://github.com/pytorch/pytorch/issues/43106\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[0;32m    573\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_legacy_zip_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_zip_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    606\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    792\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[0mdeserialized_objects\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m             \u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#embedding of data\n",
    "\n",
    "#create directory to fetch/store embedded\n",
    "embedding_dir= '../data/embeddedFiles/'\n",
    "try:\n",
    "    os.mkdir(embedding_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#try to fecth if already exist\n",
    "try:\n",
    "    with open(embedding_dir+'trainX-{}'.format(embedding)) as f:\n",
    "        X_train =  pickle.load(f)\n",
    "    with open(embedding_dir+'testX-{}'.format(embedding)) as f:\n",
    "        X_test =  pickle.load(f)\n",
    "\n",
    "#if no prior file, then embbed:\n",
    "except:\n",
    "    if embedding == \"Baseline\":\n",
    "        pass #this corresponds to original formatting\n",
    "\n",
    "    elif embedding == \"esm-1b\":\n",
    "        X_train = func.extract_sequences(X_train, merge=True).values.tolist()\n",
    "        X_train = [enc.esm_1b(seq, pooling=False) for seq in X_train]\n",
    "\n",
    "        X_test = func.extract_sequences(X_test, merge=True).values.tolist()\n",
    "        X_test = [enc.esm_1b(seq, pooling=False) for seq in X_test]\n",
    "\n",
    "        #save\n",
    "        with open(embedding_dir+'trainX-{}'.format(embedding)) as f:\n",
    "            pickle.dump(X_train, f)\n",
    "        with open(embedding_dir+'testX-{}'.format(embedding)) as f:\n",
    "            pickle.dump(X_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed hyperparameters\n",
    "features = list(range(54))  #to be redefined\n",
    "residues = list(range(416))  #to be redefined\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "n_features = len(features)\n",
    "input_size = len(residues)\n",
    "bat_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data load\n",
    "\n",
    "X_train = np.concatenate(data_list[ :-1])\n",
    "y_train = np.concatenate(target_list[:-1])\n",
    "nsamples, nx, ny = X_train.shape\n",
    "print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "X_valid = np.concatenate(data_list[-1: ])\n",
    "y_valid = np.concatenate(target_list[-1: ])\n",
    "nsamples, nx, ny = X_valid.shape\n",
    "print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "# Dataloader\n",
    "train_ds = []\n",
    "for i in range(len(X_train)):\n",
    "    train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "val_ds = []\n",
    "for i in range(len(X_valid)):\n",
    "    val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize network\n",
    "net = Net_project(num_classes=num_classes, \n",
    "             n_features=n_features, \n",
    "             numHN=numHN, \n",
    "             numFilter=numFilter,\n",
    "             dropOutRate=dropOutRate).to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate,\n",
    "                        weight_decay=0.0005,\n",
    "                        amsgrad=True,\n",
    "                        )\n",
    "\n",
    "#train\n",
    "train_acc, train_losses, train_auc, valid_acc, valid_losses, valid_auc, val_preds, val_targs = func.train_th(net, optimizer, train_ldr, val_ldr, [], [], epochs, criterion)\n",
    "\n",
    "#-------- Performance --------#\n",
    "epoch = np.arange(1,len(train_losses)+1)\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_losses, 'r', epoch, valid_losses, 'b')\n",
    "plt.legend(['Train Loss','Validation Loss'])\n",
    "plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "\n",
    "epoch = np.arange(1,len(train_auc)+1)\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_auc, 'r', epoch, valid_auc, 'b')\n",
    "plt.legend(['Train AUC','Validation AUC'])\n",
    "plt.xlabel('Epoch'), plt.ylabel('AUC')\n",
    "\n",
    "epoch = np.arange(1,len(train_acc)+1)\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "plt.legend(['Train Accuracy','Validation Accuracy'])\n",
    "plt.xlabel('Epoch'), plt.ylabel('Acc')\n",
    "plt.show()\n",
    "\n",
    "#save results\n",
    "results = pd.DataFrame(list(zip( (int(x) for x in val_targs), (int(x) for x in val_preds))),columns =['target', 'pred'])\n",
    "print(results)\n",
    "\n",
    "#metrics\n",
    "AUC = roc_auc_score(results['target'], results['pred'])\n",
    "MCC = matthews_corrcoef(results['target'], results['pred'])\n",
    "print(\"AUC: \", AUC)\n",
    "print(\"MCC: \", MCC)\n",
    "\n",
    "confusion_matrix = pd.crosstab(results['target'], results['pred'], rownames=['Actual'], colnames=['Predicted'])\n",
    "sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "plt.show()\n",
    "\n",
    "print( len([i for i, (a, b) in enumerate(zip(results['pred'], results['target'])) if a != b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing values\n",
    "with mlflow.start:run():\n",
    "    mlflow.log_param('embedding', embedding) \n",
    "    mlflow.log_param('Hidden Neurons', numHN)\n",
    "    mlflow.log_param('filters CNN', numFilter)\n",
    "    mlflow.log_param('Dropout rate', dropOutRate)\n",
    "    mlflow.log_metric('AUC', AUC)\n",
    "    mlflow.log_metric('MCC', MCC)\n",
    "                                        #ADD ARTIFACTS (PLOTS)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
