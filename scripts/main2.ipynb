{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a01d791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Libraries --------#\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3423c37a-4b59-4a1f-93f8-4ef70f490869",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dayad\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-af7af4354396>\", line 3, in <module>\n",
      "    from model import Net, Net_th, Net_project\n",
      "ImportError: cannot import name 'Net_project' from 'model' (C:\\Users\\dayad\\Desktop\\dev\\02456_TCR-pMHC\\scripts\\model.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dayad\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ImportError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dayad\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\dayad\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\dayad\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\dayad\\Anaconda3\\lib\\inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\dayad\\Anaconda3\\lib\\inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\dayad\\Anaconda3\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\dayad\\Anaconda3\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\dayad\\Anaconda3\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-af7af4354396>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0menc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNet_th\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNet_project\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Net_project' from 'model' (C:\\Users\\dayad\\Desktop\\dev\\02456_TCR-pMHC\\scripts\\model.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2060\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2061\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ImportError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2062\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2063\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "#-------- Import Modules from project--------#\n",
    "import encoding as enc\n",
    "from model import Net, Net_th, Net_project\n",
    "import functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4211d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Set Device --------#\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPUs available. Using CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5adc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Seeds --------#\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f33ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Directories --------#\n",
    "\n",
    "DATADIR = '/data/'\n",
    "TRAINDIR = '../data/train'\n",
    "VALIDATIONDIR = '../data/validation'\n",
    "MATRICES = '/data/Matrices'\n",
    "#plot_name = '{}'.format(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f45575",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Unzip Train --------#\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(TRAINDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(TRAINDIR))\n",
    "except:\n",
    "    !unzip ../data/train.zip -d ../data/train\n",
    "\n",
    "    \n",
    "#-------- Unzip Validation --------#\n",
    "\n",
    "\n",
    "try:\n",
    "    if len(os.listdir(VALIDATIONDIR)) != 0:\n",
    "        print(\"{} already unzipped.\".format(VALIDATIONDIR))\n",
    "except:\n",
    "    !unzip ../data/validation.zip -d ../data/validation\n",
    "    \n",
    "print('Train directory:\\n\\n', '\\n'.join(str(p) for p in os.listdir(TRAINDIR)), '\\n\\n')\n",
    "print('Validation directory:\\n\\n','\\n'.join(str(p) for p in os.listdir(VALIDATIONDIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b37f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Dataset --------#             #TO UPDATE: NO REPEAT P4 AND CONSERVE PARTITION FOR CROSS VAL\n",
    "\n",
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "import glob\n",
    "for fp in glob.glob(\"../data/train/*input.npz\"):\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "print(len(data_list))\n",
    "print(len(target_list))\n",
    "\n",
    "#TO ERASE LATER\n",
    "#X_train = np.concatenate(data_list[ :-1])\n",
    "#y_train = np.concatenate(target_list[:-1])\n",
    "#nsamples, nx, ny = X_train.shape\n",
    "#print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "#X_val = np.concatenate(data_list[-1: ])\n",
    "#y_val = np.concatenate(target_list[-1: ])\n",
    "#nsamples, nx, ny = X_val.shape\n",
    "#print(\"Val set shape:\", nsamples,nx,ny)\n",
    "\n",
    "#p_neg = len(y_train[y_train == 1])/len(y_train)*100\n",
    "#print(\"Percent positive samples in train:\", p_neg)\n",
    "\n",
    "#p_pos = len(y_val[y_val == 1])/len(y_val)*100\n",
    "#print(\"Percent positive samples in val:\", p_pos)\n",
    "\n",
    "# make the data set into one dataset that can go into dataloader\n",
    "#train_ds = []\n",
    "#for i in range(len(X_train)):\n",
    "#    train_ds.append([np.transpose(X_train[i]), y_train[i]])\n",
    "\n",
    "#val_ds = []\n",
    "#for i in range(len(X_val)):\n",
    "#    val_ds.append([np.transpose(X_val[i]), y_val[i]])\n",
    "\n",
    "#bat_size = 64\n",
    "#print(\"\\nNOTE:\\nSetting batch-size to\", bat_size)\n",
    "#train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "#val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9897fc5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#-------- Select the network you would like to use -------#\n",
    "\n",
    "CNN = False # ONLY CNN\n",
    "CNN_RNN = True # CNN + RNN\n",
    "\n",
    "# Hyperparameters to fine-tune\n",
    "embedding = \"Baseline\"\n",
    "numHN=64\n",
    "numFilter=100\n",
    "dropOutRate=0.1\n",
    "keep_energy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46dc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding of data\n",
    "\n",
    "#create directory to fetch/store embedded\n",
    "embedding_dir= '../data/embeddedFiles/'\n",
    "try:\n",
    "    os.mkdir(embedding_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#try to fecth if already exist\n",
    "try:\n",
    "    \n",
    "    with open(embedding_dir+'dataset-{}'.format(embedding)) as f:\n",
    "        data_list_enc =  pickle.load(f)\n",
    "\n",
    "\n",
    "#if no prior file, then embbed:\n",
    "except:\n",
    "    data_list_enc = []\n",
    "    if embedding == \"Baseline\":\n",
    "        data_list_enc = data_list\n",
    "    \n",
    "    elif embedding == \"esm-1b\":\n",
    "        for dataset in data_list:\n",
    "            x_enc = func.extract_sequences(dataset, merge=True).values.tolist()\n",
    "            print(data_list_enc)\n",
    "            x_enc = [enc.esm_1b(seq, pooling=False) for seq in x_enc]\n",
    "            data_list_enc.append(x_enc)\n",
    "            \n",
    "        #save\n",
    "        outfile = open(embedding_dir+'dataset-{}'.format(embedding),'w')\n",
    "        pickle.dump(data_list_enc, f)\n",
    "            \n",
    "    else:         \n",
    "        for dataset in data_list:\n",
    "            x_enc = func.extract_sequences(dataset, merge=True).values.tolist()\n",
    "            print(data_list_enc)\n",
    "            x_enc = [enc.encodePeptidesCNN(seq, scheme=embedding) for seq in x_enc]\n",
    "            data_list_enc.append(x_enc)\n",
    "        print(data_list_enc)\n",
    "        #save\n",
    "        with open(embedding_dir+'dataset-{}'.format(embedding)) as f:\n",
    "            pickle.dump(data_list_enc, f)\n",
    "\n",
    "            \n",
    "\"\"\"# IN process          \n",
    "if keep_energy:\n",
    "    if embedding == \"Baseline\":\n",
    "        pass\n",
    "    else:\n",
    "        for i in range (len(data_list_enc)):\n",
    "            energy_set = extract_energy_terms(data_list[i]) \n",
    "\"\"\" \n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c8d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixed hyperparameters\n",
    "features = list(range(54))  #to be redefined\n",
    "residues = list(range(416))  #to be redefined\n",
    "num_classes = 1\n",
    "learning_rate = 0.001\n",
    "n_features = len(features)\n",
    "input_size = len(residues)\n",
    "bat_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86cba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data load\n",
    "\n",
    "X_train = np.concatenate(data_list_enc[ :-1])\n",
    "y_train = np.concatenate(target_list[:-1])\n",
    "nsamples, nx, ny = X_train.shape\n",
    "print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "X_valid = np.concatenate(data_list_enc[-1: ])\n",
    "y_valid = np.concatenate(target_list[-1: ])\n",
    "nsamples, nx, ny = X_valid.shape\n",
    "print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "# Dataloader\n",
    "train_ds = []\n",
    "for i in range(len(X_train)):\n",
    "    train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "val_ds = []\n",
    "for i in range(len(X_valid)):\n",
    "    val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4c5cfd-f839-4575-a8ae-eb58a5b4bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "###    CNN Code (hecaton)   ###\n",
    "###############################\n",
    "\n",
    "if CNN:\n",
    "    \n",
    "    #-------- Define network --------#\n",
    "\n",
    "    print(\"Initializing network\")\n",
    "\n",
    "    # Hyperparameters\n",
    "    input_size = 420\n",
    "    num_classes = 1\n",
    "    learning_rate = 0.01\n",
    "    epochs = 10\n",
    "\n",
    "    # Initialize network\n",
    "    net = Net(num_classes=num_classes).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "    #-------- Train network --------#\n",
    "\n",
    "    print(\"Training\")\n",
    "\n",
    "    train_acc, train_loss = [], []\n",
    "    valid_acc, valid_loss = [], []\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        cur_loss = 0\n",
    "        val_loss = 0\n",
    "\n",
    "        net.train()\n",
    "        train_preds, train_targs = [], [] \n",
    "        for batch_idx, (data, target) in enumerate(train_ldr):\n",
    "            X_batch =  data.float().detach().requires_grad_(True)\n",
    "            target_batch = torch.tensor(np.array(target), dtype = torch.float).unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = net(X_batch)\n",
    "\n",
    "            batch_loss = criterion(output, target_batch)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = np.round(output.detach().cpu())\n",
    "            train_targs += list(np.array(target_batch.cpu()))\n",
    "            train_preds += list(preds.data.numpy().flatten())\n",
    "            cur_loss += batch_loss.detach()\n",
    "\n",
    "        losses.append(cur_loss / len(train_ldr.dataset))\n",
    "        net.eval()\n",
    "\n",
    "        ### Evaluate validation\n",
    "        val_preds, val_targs = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_ldr): ###\n",
    "                x_batch_val = data.float().detach()\n",
    "                y_batch_val = target.float().detach().unsqueeze(1)\n",
    "\n",
    "                output = net(x_batch_val)\n",
    "\n",
    "                val_batch_loss = criterion(output, y_batch_val)\n",
    "\n",
    "                preds = np.round(output.detach())\n",
    "                val_preds += list(preds.data.numpy().flatten()) \n",
    "                val_targs += list(np.array(y_batch_val))\n",
    "                val_loss += val_batch_loss.detach()\n",
    "\n",
    "            val_losses.append(val_loss / len(val_ldr.dataset))\n",
    "            print(\"\\nEpoch:\", epoch+1)\n",
    "\n",
    "            train_acc_cur = accuracy_score(train_targs, train_preds)  \n",
    "            valid_acc_cur = accuracy_score(val_targs, val_preds) \n",
    "\n",
    "            train_acc.append(train_acc_cur)\n",
    "            valid_acc.append(valid_acc_cur)\n",
    "\n",
    "            print(\"Training loss:\", round(losses[-1].item(),3), \"Validation loss:\", round(val_losses[-1].item(),3), end = \"\\n\")\n",
    "            print(\"MCC Train:\", round(matthews_corrcoef(train_targs, train_preds),3), \"MCC val:\", round(matthews_corrcoef(val_targs, val_preds),3) )\n",
    "\n",
    "    print('\\nFinished Training ...')\n",
    "\n",
    "\n",
    "    #-------- Save results --------#\n",
    "\n",
    "    results = pd.DataFrame(list(zip( (x[0] for x in val_targs), val_preds)),columns =['target', 'pred'])\n",
    "    print(results)\n",
    "\n",
    "    #results.to_csv('results/df_targets_preds.csv'.format(str(date.today())), index=False)\n",
    "\n",
    "\n",
    "    #-------- Performance Evaluation --------#\n",
    "    # The results change every time we train, we should check why (maybe we missed something or did wrong with the seeds?)\n",
    "\n",
    "    print(\"AUC: \", roc_auc_score(results['target'], results['pred']))\n",
    "    print(\"MCC: \", matthews_corrcoef(results['target'], results['pred']))\n",
    "\n",
    "    confusion_matrix = pd.crosstab(results['target'], results['pred'], rownames=['Actual'], colnames=['Predicted'])\n",
    "    sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1153ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################\n",
    "###    CNN+RNN (thesis)     ###\n",
    "###############################\n",
    "\n",
    "if CNN_RNN:\n",
    "    \n",
    "    #-------- Initials --------#\n",
    "\n",
    "    # All features\n",
    "    features = list(range(54))\n",
    "    residues = list(range(416))\n",
    "    input_size = len(residues)\n",
    "    num_classes = 1\n",
    "    learning_rate = 0.001\n",
    "    bat_size = 128\n",
    "    epochs = 100\n",
    "    n_features = len(features)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "\n",
    "    #-------- Train --------#\n",
    "    nsamples, nx, ny = X_train.shape\n",
    "    print(\"Training set shape:\", nsamples,nx,ny)\n",
    "\n",
    "    X_valid = X_val\n",
    "    nsamples, nx, ny = X_valid.shape\n",
    "    print(\"Validation set shape:\", nsamples,nx,ny)\n",
    "\n",
    "    # Dataloader\n",
    "    train_ds = []\n",
    "    for i in range(len(X_train)):\n",
    "        train_ds.append([np.transpose(X_train[i][:,features]), y_train[i]])\n",
    "    val_ds = []\n",
    "    for i in range(len(X_valid)):\n",
    "        val_ds.append([np.transpose(X_valid[i][:,features]), y_valid[i]])\n",
    "    train_ldr = torch.utils.data.DataLoader(train_ds,batch_size=bat_size, shuffle=True)\n",
    "    val_ldr = torch.utils.data.DataLoader(val_ds,batch_size=bat_size, shuffle=True)\n",
    "\n",
    "    # Initialize network\n",
    "    net = Net_project(num_classes=num_classes, \n",
    "             n_features=n_features, \n",
    "             numHN=numHN, \n",
    "             numFilter=numFilter,\n",
    "             dropOutRate=dropOutRate).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate,\n",
    "                           weight_decay=0.0005,\n",
    "                           amsgrad=True,)\n",
    "    \n",
    "    train_acc, train_losses, train_auc, valid_acc, valid_losses, valid_auc, val_preds, val_targs = func.train_project(net, optimizer, train_ldr, val_ldr, [], X_valid, epochs, criterion)\n",
    "\n",
    "\n",
    "    #-------- Performance --------#\n",
    "    epoch = np.arange(1,len(train_losses)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_losses, 'r', epoch, valid_losses, 'b')\n",
    "    plt.legend(['Train Loss','Validation Loss'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "\n",
    "    epoch = np.arange(1,len(train_auc)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_auc, 'r', epoch, valid_auc, 'b')\n",
    "    plt.legend(['Train AUC','Validation AUC'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('AUC')\n",
    "\n",
    "    epoch = np.arange(1,len(train_acc)+1)\n",
    "    plt.figure()\n",
    "    plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "    plt.legend(['Train Accuracy','Validation Accuracy'])\n",
    "    plt.xlabel('Epoch'), plt.ylabel('Acc')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    #-------- Save results --------#\n",
    "\n",
    "    results = pd.DataFrame(list(zip( (int(x) for x in val_targs), (int(x) for x in val_preds))),columns =['target', 'pred'])\n",
    "    print(results)\n",
    "\n",
    "    results.to_csv('results/df_targets_preds_th.csv'.format(str(date.today())), index=False\n",
    "    \n",
    "    \n",
    "    #-------- Performance Evaluation --------#\n",
    "    # The results change every time we train, we should check why (maybe we missed something or did wrong with the seeds?)\n",
    "\n",
    "    print(\"AUC: \", roc_auc_score(results['target'], results['pred']))\n",
    "    print(\"MCC: \", matthews_corrcoef(results['target'], results['pred']))\n",
    "\n",
    "    confusion_matrix = pd.crosstab(results['target'], results['pred'], rownames=['Actual'], colnames=['Predicted'])\n",
    "    sn.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot roc curve\n",
    "\n",
    "    fpr, tpr, thres = roc_curve(results['target'], results['pred'])\n",
    "    print('AUC: {:.3f}'.format(roc_auc_score(results['target'], results['pred'])))\n",
    "\n",
    "    print( len([i for i, (a, b) in enumerate(zip(results['pred'], results['target'])) if a != b]))\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    # roc curve\n",
    "    plt.plot(fpr, tpr, \"b\", label='ROC Curve')\n",
    "    plt.plot([0,1],[0,1], \"k--\", label='Random Guess')\n",
    "    plt.xlabel(\"false positive rate\")\n",
    "    plt.ylabel(\"true positive rate\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"ROC curve\")\n",
    "\n",
    "    #plt.savefig(plot_name + '_roc.png')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75bb91f-7dde-48da-a051-d6a5acf9b6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metrics\n",
    "AUC = roc_auc_score(results['target'], results['pred'])\n",
    "MCC = matthews_corrcoef(results['target'], results['pred'])\n",
    "print(\"AUC: \", AUC)\n",
    "print(\"MCC: \", MCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing values\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param('embedding', embedding) \n",
    "    mlflow.log_param('Hidden Neurons', numHN)\n",
    "    mlflow.log_param('filters CNN', numFilter)\n",
    "    mlflow.log_param('Dropout rate', dropOutRate)\n",
    "    mlflow.log_metric('AUC', AUC)\n",
    "    mlflow.log_metric('MCC', MCC)\n",
    "    #ADD ARTIFACTS (PLOTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebcc79-4feb-4f4f-bf49-0271c1dcacac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
