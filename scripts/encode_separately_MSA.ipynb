{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "688fd78d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-------- Import Libraries --------#\n",
    "import torchvision\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import esm\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import mlflow\n",
    "import gc\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torch.nn.functional as F  # All functions that don't have any parameters\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05ce9493",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Modules from project--------#\n",
    "import functions as func\n",
    "#import encoding as enc\n",
    "\n",
    "# Load pre-trained ESM-MSA-1b model\n",
    "model, alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "    \n",
    "## NO PADDING:\n",
    "def esm_MSA(peptide, pooling=False, add_padding=True):\n",
    "    \n",
    "    peptides = [peptide]\n",
    "    \n",
    "    embeddings = list()\n",
    "    # Load pre-trained ESM-1b model\n",
    "            #above\n",
    "        \n",
    "    data = []\n",
    "    \n",
    "    for peptide in peptides:\n",
    "        data.append((\"\", peptide))\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[12], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][12].numpy()[0][0]\n",
    "        \n",
    "    del results, batch_labels, batch_strs, batch_tokens\n",
    "    gc.collect()\n",
    "    \n",
    "    sequence_representations = []\n",
    "    \n",
    "    for i, (_, seq) in enumerate(data):\n",
    "\n",
    "        if pooling:\n",
    "            sequence_representations = token_representations[1:, ].mean(0)\n",
    "\n",
    "        else:\n",
    "            sequence_representations = token_representations[1:, ]\n",
    "            if add_padding:\n",
    "                pad = 420 - sequence_representations.shape[0]\n",
    "                sequence_representations = np.pad(sequence_representations, ((0, pad), (0, 0)), 'constant')\n",
    "        \n",
    "    del token_representations\n",
    "    gc.collect()\n",
    "    \n",
    "    return sequence_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f335584e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPUs available. Using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "#-------- Set Device --------#\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "else:\n",
    "    print('No GPUs available. Using CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60a9e777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Seeds --------#\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ff35fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------- Import Modules from project--------#\n",
    "\n",
    "import encoding as enc\n",
    "from model import Net_project\n",
    "import functions as func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3df3c4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file ../data/train/P1_input.npz\n",
      "Read file ../data/train/P2_input.npz\n",
      "Read file ../data/train/P3_input.npz\n",
      "Read file ../data/train/P4_input.npz\n",
      "Read file ../data/validation/P5_input.npz\n",
      "\n",
      "\n",
      "data_list: 5\n",
      "target_list 5\n",
      "\n",
      "\n",
      "Size of file 1 1526\n",
      "Size of file 2 1168\n",
      "Size of file 3 1480\n",
      "Size of file 4 1532\n",
      "Size of file 5 1207\n"
     ]
    }
   ],
   "source": [
    "#-------- Import Dataset --------#      \n",
    "\n",
    "data_list = []\n",
    "target_list = []\n",
    "\n",
    "import glob\n",
    "for index in range(5):\n",
    "    for fp in glob.glob(\"../data/train/*{}*input.npz\".format(index+1)):\n",
    "        print(\"Read file\", fp)\n",
    "        data = np.load(fp)[\"arr_0\"]\n",
    "        targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "        data_list.append(data)\n",
    "        target_list.append(targets)\n",
    "    \n",
    "for fp in glob.glob(\"../data/validation/*5*input.npz\"):\n",
    "    print(\"Read file\", fp)\n",
    "    data = np.load(fp)[\"arr_0\"]\n",
    "    targets = np.load(fp.replace(\"input\", \"labels\"))[\"arr_0\"]\n",
    "    data_list.append(data)\n",
    "    target_list.append(targets)\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"data_list:\", len(data_list))\n",
    "print(\"target_list\", len(target_list))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "data_partitions = len(data_list)\n",
    "for i in range(data_partitions):\n",
    "    print(\"Size of file\", i+1, len(data_list[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding of data\n",
    "embedding = 'MSA'\n",
    "files_complete = False\n",
    "merge = False\n",
    "\n",
    "data_list_enc = list()\n",
    "\n",
    "start = time.time()\n",
    "mhc_bool = False\n",
    "\n",
    "#create directory to fetch/store embedded\n",
    "\n",
    "if embedding == \"baseline\":\n",
    "    print('baseline')\n",
    "    data_list_enc = data_list\n",
    "\n",
    "elif embedding == \"MSA\":\n",
    "    if files_complete == False:\n",
    "        print(\"MSA\")\n",
    "        count = 0\n",
    "\n",
    "        for dataset in data_list:\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            print(\"\\nWorking on file\", count)\n",
    "            \n",
    "            mhc_enc = list()\n",
    "            pep_enc = list()\n",
    "            tcr_enc = list()\n",
    "\n",
    "            x_enc = func.extract_sequences(dataset, merge = merge)\n",
    "            \n",
    "            print(\"Sequences are extracted\")\n",
    "\n",
    "            if merge:\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "                print(\"Merge false ( means Separate True )\")\n",
    "                print(len(dataset), len(x_enc['MHC'].tolist()), len(x_enc['peptide'].tolist()), len(x_enc['tcr'].tolist()))\n",
    "\n",
    "                if mhc_bool == False:\n",
    "                    print(\"\\nEncode MHC\")\n",
    "                    mhc_enc_1 = esm_MSA(x_enc['MHC'].tolist()[0], pooling=False, add_padding=False)\n",
    "                    print(\"Done:\", len(mhc_enc_1))\n",
    "                    mhc_bool = True\n",
    "\n",
    "                for i in range(len(x_enc['MHC'].tolist())):\n",
    "                    if i % 10 == 0:\n",
    "                        print(\"\\nFlag\", i, \"peptides are encoded - Time:\", round((time.time()-start)/60,2), \"mins.\")\n",
    "\n",
    "                    mhc_enc.append(mhc_enc_1)\n",
    "                    pep_enc.append(esm_MSA(x_enc['peptide'].tolist()[i], pooling=False, add_padding=False))\n",
    "                    tcr_enc.append(esm_MSA(x_enc['tcr'].tolist()[i], pooling=False, add_padding=False))\n",
    "\n",
    "                mhc_enc = [x.tolist() for x in mhc_enc]\n",
    "                pep_enc = [x.tolist() for x in pep_enc]\n",
    "                tcr_enc = [x.tolist() for x in tcr_enc]\n",
    "                \n",
    "                print(\"ESM_1B is done.\\n\")\n",
    "                print(\"Wriring in files.\\n\")\n",
    "\n",
    "                # save separate encodings:\n",
    "                \n",
    "                outfilemhc = open(embedding_dir + 'dataset-{}-file{}-mhc-MSA.pkl'.format(embedding, count),'wb')\n",
    "                outfilepep = open(embedding_dir + 'dataset-{}-file{}-pep-MSA.pkl'.format(embedding, count),'wb')\n",
    "                outfiletcr = open(embedding_dir + 'dataset-{}-file{}-tcr-MSA.pkl'.format(embedding, count),'wb')\n",
    "                pickle.dump(mhc_enc, outfilemhc)\n",
    "                pickle.dump(pep_enc, outfilepep)\n",
    "                pickle.dump(tcr_enc, outfiletcr)                \n",
    "                outfilemhc.close()\n",
    "                outfilepep.close()\n",
    "                outfiletcr.close()\n",
    "                \n",
    "                print(\"Wriring in files is done.\\n\")\n",
    "                \n",
    "\n",
    "                ## PREPARE df to paste here\n",
    "                \n",
    "                print(\"create df_aa\")\n",
    "                df_aa = func.extract_aa_and_energy_terms(dataset)\n",
    "                print(\"df_aa is done\")\n",
    "                \n",
    "                ### paste energy terms here\n",
    "                \n",
    "                data_list_PASTE = list()\n",
    "\n",
    "                for cmplx in range(len(dataset)):\n",
    "\n",
    "                        df = pd.DataFrame( dataset[cmplx] )\n",
    "                        new_df = pd.DataFrame( df_aa[cmplx] )\n",
    "\n",
    "                        df_emb_mhc = pd.DataFrame(mhc_enc[cmplx])\n",
    "                        df_emb_pep = pd.DataFrame(pep_enc[cmplx])\n",
    "                        df_emb_tcr = pd.DataFrame(tcr_enc[cmplx])\n",
    "\n",
    "                        pad_index_list = sorted(new_df[new_df.iloc[:,34]=='X'].index.tolist())\n",
    "\n",
    "                        padding1_len = 0\n",
    "                        for pad in range(len(pad_index_list)):\n",
    "                            padding1_len += 1\n",
    "                            if pad_index_list[-1] > 230:\n",
    "                                if pad_index_list[pad+1]-pad_index_list[pad] > 100:\n",
    "                                    tcr_start = pad_index_list[pad] + 1\n",
    "                                    tcr_end = pad_index_list[pad+1]\n",
    "                                    break\n",
    "                            else:\n",
    "                                tcr_start = pad_index_list[-1] + 1\n",
    "                                tcr_end = 420\n",
    "\n",
    "                        padding2_len = len(pad_index_list) - padding1_len\n",
    "\n",
    "                        mhc = pd.concat([df_emb_mhc.reset_index(drop=True), df.iloc[:179,20:].reset_index(drop=True)], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "                        pep = pd.concat([df_emb_pep.reset_index(drop=True), df.iloc[179:188,20:].reset_index(drop=True)], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "                        padding1 = pd.DataFrame(0, index=np.arange(padding1_len), columns=pep.columns)\n",
    "                        tcr = pd.concat([df_emb_tcr.reset_index(drop=True), (df.iloc[tcr_start:tcr_end,20:]).reset_index(drop=True)], axis=1).iloc[:, :-1].reset_index(drop=True)\n",
    "                        padding2 = pd.DataFrame(0, index=np.arange(padding2_len), columns=pep.columns)\n",
    "\n",
    "                        final_cmplx = pd.concat([mhc,pep,padding1,tcr,padding2]).reset_index(drop=True).values\n",
    "\n",
    "                        data_list_PASTE.append( final_cmplx )\n",
    "\n",
    "                        if cmplx % 100 == 0:\n",
    "                            print(\"Flag - pasting:\", cmplx)\n",
    "                            print(len(final_cmplx))\n",
    "\n",
    "                o = open('esm-energies-file-MSA-{}.pkl'.format(count),'wb')\n",
    "                pickle.dump(data_list_PASTE, o)\n",
    "                o.close()\n",
    "                \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5412c81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
